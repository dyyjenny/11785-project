{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Use biopython library to process fasta files\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in fasta file\n",
    "\n",
    "fasta_filename = \"data/spikeprot0309.fasta\"\n",
    "fasta_entries = SeqIO.parse(fasta_filename, \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of FASTA entries:  702408\n",
      "\n",
      "Example FASTA entry: \n",
      "{'Strain': 'Wuhan/WIV04/2019', 'Submission Date': '2019-12-30', 'EPI_ISL': 'EPI_ISL_402124', 'Division of Exposure': 'hCoV-19^^Hubei', 'Originating Lab': 'Wuhan Jinyintan Hospital', 'Submitting Lab': 'Wuhan Institute of Virology', 'Author': 'Shi', 'Country of Exposure': 'China', 'Sequence': 'MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIVNNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLEGKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSALEPLVDLPIGINITRFQTLLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNENGTITDAVDCALDPLSETKCTLKSFTVEKGIYQTSNFRVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVNFNFNGLTGTGVLTESNKKFLPFQQFGRDIADTTDAVRDPQTLEILDITPCSFGGVSVITPGTNTSNQVAVLYQDVNCTEVPVAIHADQLTPTWRVYSTGSNVFQTRAGCLIGAEHVNNSYECDIPIGAGICASYQTQTNSPRRARSVASQSIIAYTMSLGAENSVAYSNNSIAIPTNFTISVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQLNRALTGIAVEQDKNTQEVFAQVKQIYKTPPIKDFGGFNFSQILPDPSKPSKRSFIEDLLFNKVTLADAGFIKQYGDCLGDIAARDLICAQKFNGLTVLPPLLTDEMIAQYTSALLAGTITSGWTFGAGAALQIPFAMQMAYRFNGIGVTQNVLYENQKLIANQFNSAIGKIQDSLSSTASALGKLQDVVNQNAQALNTLVKQLSSNFGAISSVLNDILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRASANLAATKMSECVLGQSKRVDFCGKGYHLMSFPQSAPHGVVFLHVTYVPAQEKNFTTAPAICHDGKAHFPREGVFVSNGTHWFVTQRNFYEPQIITTDNTFVSGNCDVVIGIVNNTVYDPLQPELDSFKEELDKYFKNHTSPDVDLGDISGINASVVNIQKEIDRLNEVAKNLNESLIDLQELGKYEQYIKWPWYIWLGFIAGLIAIVMVTIMLCCMTSCCSCLKGCCSCGSCCKFDEDDSEPVLKGVKLHYT*'}\n"
     ]
    }
   ],
   "source": [
    "# Process each fasta sample into a dictionary\n",
    "# The key is the EPI_ISL ID, the value is a dictionary with metadata and the protein sequence\n",
    "\n",
    "fasta_samples = {}\n",
    "for entry in fasta_entries:\n",
    "    metadata_list = entry.description.split(\"|\")\n",
    "    protein_sequence = str(entry.seq)\n",
    "    \n",
    "    # Account for rows with missing data\n",
    "    while len(metadata_list) < 11:\n",
    "        metadata_list.append(\"\")\n",
    "        \n",
    "    # Remove \"hCoV-19/\" prefix and remove spaces from fasta strain\n",
    "    strain = metadata_list[1]\n",
    "    if strain[:8] == \"hCoV-19/\":\n",
    "        strain = strain[8:]\n",
    "    strain = strain.replace(\" \", \"\")\n",
    "        \n",
    "    # Create dictionary from entry\n",
    "    sample = {\n",
    "        \"Strain\": strain,\n",
    "        \"Submission Date\": metadata_list[2],\n",
    "        \"EPI_ISL\": metadata_list[3],\n",
    "        \"Division of Exposure\": metadata_list[5],\n",
    "        \"Originating Lab\": metadata_list[7],\n",
    "        \"Submitting Lab\": metadata_list[8],\n",
    "        \"Author\": metadata_list[9],\n",
    "        \"Country of Exposure\": metadata_list[10],\n",
    "        \"Sequence\": protein_sequence,\n",
    "    }\n",
    "    \n",
    "    # Add sample to fasta_samples\n",
    "    epi_isl = metadata_list[3]\n",
    "    assert(epi_isl[:7] == \"EPI_ISL\")\n",
    "    fasta_samples[epi_isl] = sample\n",
    "    \n",
    "print(\"Number of FASTA entries: \", len(fasta_samples.keys()))\n",
    "print(\"\")\n",
    "print(\"Example FASTA entry: \")\n",
    "for (_, value) in fasta_samples.items():\n",
    "    print(value)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of metadata entries:  3860\n",
      "\n",
      "Example metadata entry: \n",
      "{'Strain': 'Wuhan/WH01/2019', 'GISAID Clade': 'L', 'S1 mutations': 0, 'Age': '44', 'Clade': '19A', 'Country': 'China', 'Country of Exposure': 'China', 'Admin Division': 'Hubei', 'Division of Exposure': 'Hubei', 'genbank_accession': 'LR757998.1', 'gisaid_epi_isl': 'EPI_ISL_406798', 'Host': 'Human', 'Location': 'Wuhan', 'Originating Lab': \"General Hospital of Central Theater Command of People's Liberation Army of China\", 'PANGO Lineage': 'B', 'Submission Date': 'Older', 'Region': 'Asia', 'Sex': 'Male', 'Emerging Clade': '19A', 'Submitting Lab': \"BGI & Institute of Microbiology, Chinese Academy of Sciences & Shandong First Medical University & Shandong Academy of Medical Sciences & General Hospital of Central Theater Command of People's Liberation Army of China\", 'url': '', 'Collection Data': '2019-12-26', 'Author': 'Weijun Chen et al (https://dx.doi.org/10.1016/S0140-6736(20)30251-8)', 'Region of Exposure': ''}\n"
     ]
    }
   ],
   "source": [
    "# Process nextstrain global metadata into a dictionary\n",
    "\n",
    "metadata_filename = \"data/nextstrain_ncov_global_metadata.tsv\"\n",
    "metadata_samples = {}\n",
    "with open(metadata_filename, \"r\") as f:\n",
    "    tsv_reader = csv.reader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    header = next(tsv_reader)\n",
    "    \n",
    "    for row in tsv_reader:\n",
    "        # Create dictionary from row\n",
    "        sample = {}\n",
    "        for i in range(len(row)):\n",
    "            sample[header[i]] = row[i]\n",
    "            \n",
    "        # if s1 mutation is empty, means no mutation\n",
    "        if sample['S1 mutations'] == '':\n",
    "            sample['S1 mutations'] = 0\n",
    "        sample['S1 mutations']  = int(float(sample['S1 mutations']))\n",
    "            \n",
    "            \n",
    "        # Add sample to metadata_samples\n",
    "        epi_isl = row[10]\n",
    "        assert(epi_isl[:7] == \"EPI_ISL\")\n",
    "        metadata_samples[epi_isl] = sample\n",
    "        \n",
    "    print(\"Number of metadata entries: \", len(metadata_samples.keys()))\n",
    "    print(\"\")\n",
    "    print(\"Example metadata entry: \")\n",
    "    for (_, value) in metadata_samples.items():\n",
    "        print(value)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching EPI_ISL ID's between NextStrain metadata and FASTA:  2488\n"
     ]
    }
   ],
   "source": [
    "# Find matches between metadata and fasta\n",
    "\n",
    "metadata_fasta_matches = []\n",
    "for epi_isl in metadata_samples.keys():\n",
    "    if epi_isl in fasta_samples:\n",
    "        metadata_fasta_matches.append(epi_isl)\n",
    "        \n",
    "print(\"Number of matching EPI_ISL ID's between NextStrain metadata and FASTA: \", len(metadata_fasta_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map from amino acids and clades to ints\n",
    "\n",
    "with open(\"data/amino_list.txt\", encoding=\"utf8\") as f:\n",
    "    amino_list = f.read().strip().split(',')\n",
    "amino_codes = {}\n",
    "for (i, v) in enumerate(amino_list):\n",
    "    amino_codes[v] = i\n",
    "    \n",
    "clade_list = list(set([value[\"clade\"] for (_, value) in matched_samples.items()]))\n",
    "clade_codes = {}\n",
    "for (i, v) in enumerate(clade_list):\n",
    "    clade_codes[v] = i\n",
    "    \n",
    "gisaid_clade_list = list(set([value[\"gisaid_clade\"] for (_, value) in matched_samples.items()]))\n",
    "gisaid_clade_codes = {}\n",
    "for (i, v) in enumerate(gisaid_clade_list):\n",
    "    gisaid_clade_codes[v] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clade codes: {'20B': 0, '20H/501Y.V2': 1, '20F': 2, '20C': 3, '20J/501Y.V3': 4, '20G': 5, '20D': 6, '19A': 7, '20I/501Y.V1': 8, '20A': 9, '20E (EU1)': 10, '19B': 11}\n",
      "gisaid clade codes {'GV': 0, 'O': 1, 'GRY': 2, 'GH': 3, 'S': 4, 'GR': 5, 'L': 6, 'G': 7, 'V': 8}\n"
     ]
    }
   ],
   "source": [
    "print('clade codes:', clade_codes)\n",
    "print('gisaid clade codes', gisaid_clade_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training and validation data from string to numerical format\n",
    "\n",
    "def amino_to_num(data_list, amino_codes):\n",
    "    new_data = []\n",
    "    \n",
    "    for seq in data_list:\n",
    "        new_seq = np.array([amino_codes[char] for char in seq])\n",
    "        new_data.append(new_seq)\n",
    "        \n",
    "    return np.array(new_data, dtype=np.object)\n",
    "\n",
    "def clade_to_num(data, clade_codes):\n",
    "    new_data = [clade_codes[clade] for clade in data]\n",
    "    return np.array(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up list to store train and val data\n",
    "\n",
    "train_data_metadata_fasta_match = []\n",
    "train_label_clade = []\n",
    "train_label_gisaid_clade = []\n",
    "train_label_mutation = []\n",
    "\n",
    "validation_data_metadata_fasta_match = []\n",
    "validation_label_clade = []\n",
    "validation_label_gisaid_clade = []\n",
    "validation_label_mutation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training and validation datasets\n",
    "\n",
    "for (i, (_, value)) in enumerate(matched_samples.items()):\n",
    "    if i % 10 == 0:\n",
    "        validation_data_metadata_fasta_match.append(value[\"sequence\"])\n",
    "        validation_label_clade.append(value[\"clade\"])\n",
    "        validation_label_gisaid_clade.append(value[\"gisaid_clade\"])\n",
    "        validation_label_mutation.append(value[\"s1_mutation\"])\n",
    "    else:\n",
    "        train_data_metadata_fasta_match.append(value[\"sequence\"])\n",
    "        train_label_clade.append(value[\"clade\"])\n",
    "        train_label_gisaid_clade.append(value[\"gisaid_clade\"])\n",
    "        train_label_mutation.append(value[\"s1_mutation\"])\n",
    "        \n",
    "train_data_metadata_fasta_match_num = amino_to_num(train_data_metadata_fasta_match, amino_codes)\n",
    "train_label_clade_num = clade_to_num(train_label_clade, clade_codes)\n",
    "train_label_gisaid_clade_num = clade_to_num(train_label_gisaid_clade, gisaid_clade_codes)\n",
    "train_label_mutation = np.array(train_label_mutation)\n",
    "\n",
    "validation_data_metadata_fasta_match_num = amino_to_num(validation_data_metadata_fasta_match, amino_codes)\n",
    "validation_label_clade_num = clade_to_num(validation_label_clade, clade_codes)\n",
    "validation_label_gisaid_clade_num = clade_to_num(validation_label_gisaid_clade, gisaid_clade_codes)\n",
    "validation_label_mutation = np.array(validation_label_mutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data_updated/matched_samples_metadata_fasta.npy\", matched_samples, allow_pickle=True)\n",
    "\n",
    "np.save(\"data_updated/amino_mapping_new.npy\", amino_list, allow_pickle=True)\n",
    "np.save(\"data_updated/clade_mapping_new.npy\", clade_list, allow_pickle=True)\n",
    "np.save(\"data_updated/gisaid_clade_mapping_new.npy\", gisaid_clade_list, allow_pickle=True)\n",
    "\n",
    "np.save(\"data_updated/train_data_metadata_fasta_match.npy\", train_data_metadata_fasta_match, allow_pickle=True)\n",
    "np.save(\"data_updated/train_data_metadata_fasta_match_num.npy\", train_data_metadata_fasta_match_num, allow_pickle=True)\n",
    "np.save(\"data_updated/train_label_clade.npy\", train_label_clade, allow_pickle=True)\n",
    "np.save(\"data_updated/train_label_clade_num.npy\", train_label_clade_num, allow_pickle=True)\n",
    "np.save(\"data_updated/train_label_gisaid_clade.npy\", train_label_gisaid_clade, allow_pickle=True)\n",
    "np.save(\"data_updated/train_label_gisaid_clade_num.npy\", train_label_gisaid_clade_num, allow_pickle=True)\n",
    "np.save(\"data_updated/train_label_mutation.npy\", train_label_mutation, allow_pickle=True)\n",
    "\n",
    "np.save(\"data_updated/validation_data_metadata_fasta_match.npy\", validation_data_metadata_fasta_match, allow_pickle=True)\n",
    "np.save(\"data_updated/validation_data_metadata_fasta_match_num.npy\", validation_data_metadata_fasta_match_num, allow_pickle=True)\n",
    "np.save(\"data_updated/validation_label_clade.npy\", validation_label_clade, allow_pickle=True)\n",
    "np.save(\"data_updated/validation_label_clade_num.npy\", validation_label_clade_num, allow_pickle=True)\n",
    "np.save(\"data_updated/validation_label_gisaid_clade.npy\", validation_label_gisaid_clade, allow_pickle=True)\n",
    "np.save(\"data_updated/validation_label_gisaid_clade_num.npy\", validation_label_gisaid_clade_num, allow_pickle=True)\n",
    "np.save(\"data_updated/validation_label_mutation.npy\", validation_label_mutation, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
