{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Use biopython library to process fasta files\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each fasta sample from a given file into a dictionary\n",
    "# The key is the EPI_ISL_ID, the value is a dictionary with metadata and the protein sequence\n",
    "\n",
    "def parse_fasta(fasta_filename):\n",
    "    fasta_entries = SeqIO.parse(fasta_filename, \"fasta\")\n",
    "    fasta_samples = {}\n",
    "    \n",
    "    for entry in fasta_entries:\n",
    "        metadata_list = entry.description.split(\"|\")\n",
    "        protein_sequence = str(entry.seq)\n",
    "\n",
    "        # Account for rows with missing data\n",
    "        while len(metadata_list) < 11:\n",
    "            metadata_list.append(\"\")\n",
    "\n",
    "        # Remove \"hCoV-19/\" prefix and remove spaces from fasta strain\n",
    "        strain = metadata_list[1]\n",
    "        if strain[:8] == \"hCoV-19/\":\n",
    "            strain = strain[8:]\n",
    "        strain = strain.replace(\" \", \"\")\n",
    "\n",
    "        # Create dictionary from entry\n",
    "        sample = {\n",
    "            \"Strain\": strain,\n",
    "            \"Submission Date\": metadata_list[2],\n",
    "            \"EPI_ISL\": metadata_list[3],\n",
    "            \"Division of Exposure\": metadata_list[5],\n",
    "            \"Originating Lab\": metadata_list[7],\n",
    "            \"Submitting Lab\": metadata_list[8],\n",
    "            \"Author\": metadata_list[9],\n",
    "            \"Country of Exposure\": metadata_list[10],\n",
    "            \"Sequence\": protein_sequence,\n",
    "        }\n",
    "\n",
    "        # Add sample to fasta_samples\n",
    "        epi_isl = metadata_list[3]\n",
    "        assert(epi_isl[:7] == \"EPI_ISL\")\n",
    "        fasta_samples[epi_isl] = sample\n",
    "        \n",
    "    return fasta_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process nextstrain global metadata from a given file into a dictionary\n",
    "# The key is EPI_ISL_ID, the value is a dictionary with global metadata\n",
    "\n",
    "def parse_nextstrain_metadata(metadata_filename):\n",
    "    metadata_samples = {}\n",
    "    \n",
    "    with open(metadata_filename, \"r\") as f:\n",
    "        tsv_reader = csv.reader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "        header = next(tsv_reader)\n",
    "\n",
    "        for row in tsv_reader:\n",
    "            # Create dictionary from row\n",
    "            sample = {}\n",
    "            for i in range(len(row)):\n",
    "                sample[header[i]] = row[i]\n",
    "\n",
    "            # Add sample to metadata_samples\n",
    "            epi_isl = row[10]\n",
    "            assert(epi_isl[:7] == \"EPI_ISL\")\n",
    "            metadata_samples[epi_isl] = sample\n",
    "            \n",
    "    return metadata_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process GISAID metadata from a given file into a dictionary\n",
    "# The key is EPI_ISL_ID, the value is a dictionary with GISAID metadata\n",
    "\n",
    "def parse_gisaid_metadata(metadata_filename):\n",
    "    metadata_samples = {}\n",
    "    \n",
    "    with open(metadata_filename, \"r\") as f:\n",
    "        tsv_reader = csv.reader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "        header = next(tsv_reader)\n",
    "        \n",
    "        for row in tsv_reader:\n",
    "            # Remove \"hCoV-19/\" prefix and remove spaces from strain name\n",
    "            if row[0][:8] == \"hCoV-19/\":\n",
    "                row[0] = row[0][8:]\n",
    "            row[0] = row[0].replace(\" \", \"\")\n",
    "            \n",
    "            # Create dictionary from row\n",
    "            sample = {}\n",
    "            for i in range(len(row)):\n",
    "                sample[header[i]] = row[i]\n",
    "                \n",
    "            # Add sample to metadata_samples\n",
    "            epi_isl = row[2]\n",
    "            assert(epi_isl[:7] == \"EPI_ISL\")\n",
    "            metadata_samples[epi_isl] = sample\n",
    "            \n",
    "    return metadata_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate nextstrain global timetree data into a tree structure, and an array of timetree entries\n",
    "# Then, process global timetree entries into a dictionary\n",
    "# The key is the strain, the value is a dictionary with timetree metadata\n",
    "\n",
    "def parse_timetree(timetree_filename):\n",
    "    with open(timetree_filename, \"r\") as f:\n",
    "        timetree_str = f.read()\n",
    "        timetree_str = timetree_str[33:-6] # Remove start and end\n",
    "        \n",
    "    # Separate timetree data into a tree structure, and array of timetree entries\n",
    "    tree_structure = []\n",
    "    timetree_entries = []\n",
    "    entry_start_idx = 0\n",
    "    \n",
    "    in_strain_name = False\n",
    "    in_description = False\n",
    "    in_tree_length = False\n",
    "\n",
    "    for (i, char) in enumerate(timetree_str):\n",
    "        # Advance parser state, keeping track of when each tree entry starts and ends\n",
    "        # Parser state advances from none -> in_strain_name -> in_description -> in_tree_length\n",
    "        # Separately keep track of the tree structure, using each entry's index in timetree_entries\n",
    "        if in_strain_name:\n",
    "            if char == '[':\n",
    "                in_strain_name = False\n",
    "                in_description = True\n",
    "\n",
    "        elif in_description:\n",
    "            if char == ']':\n",
    "                in_description = False\n",
    "                in_tree_length = True\n",
    "\n",
    "        elif in_tree_length:\n",
    "            if not char.isdigit() and char != ':' and char != '.':\n",
    "                in_tree_length = False\n",
    "                timetree_entries.append(timetree_str[entry_start_idx:i])\n",
    "\n",
    "                tree_structure.append(str(len(timetree_entries)))\n",
    "                tree_structure.append(char)\n",
    "\n",
    "        else:\n",
    "            if char != ',' and char != '(' and char != ')':\n",
    "                in_strain_name = True\n",
    "                entry_start_idx = i\n",
    "            else:\n",
    "                tree_structure.append(char)\n",
    "\n",
    "    tree_structure = \"\".join(tree_structure)\n",
    "    \n",
    "    # Parse timetree entries into a dictionary\n",
    "    timetree_samples = {}\n",
    "    \n",
    "    for (timetree_id, entry) in enumerate(timetree_entries):\n",
    "        (strain, entry) = entry.split(\"[&\")\n",
    "        (entry, path_length) = entry.split(\"]:\")\n",
    "        tokens = entry.split(\",\")\n",
    "\n",
    "        # Create dictionary from entry\n",
    "        sample = {\n",
    "            \"strain\": strain,\n",
    "            \"path_length\": path_length,\n",
    "            \"timetree_id\": timetree_id,\n",
    "        }\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                (name, value) = token.split(\"=\")\n",
    "                sample[name] = value\n",
    "            except ValueError:\n",
    "                # Handle comma within num_date_CI\n",
    "                sample[\"num_date_CI\"] += \",\" + token\n",
    "\n",
    "        # Add sample to timetree_samples\n",
    "        timetree_samples[strain] = sample\n",
    "        \n",
    "    return (timetree_samples, tree_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of FASTA entries:  702408\n",
      "\n",
      "Example FASTA entry: \n",
      "{'Strain': 'Wuhan/WIV04/2019', 'Submission Date': '2019-12-30', 'EPI_ISL': 'EPI_ISL_402124', 'Division of Exposure': 'hCoV-19^^Hubei', 'Originating Lab': 'Wuhan Jinyintan Hospital', 'Submitting Lab': 'Wuhan Institute of Virology', 'Author': 'Shi', 'Country of Exposure': 'China', 'Sequence': 'MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIVNNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLEGKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSALEPLVDLPIGINITRFQTLLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNENGTITDAVDCALDPLSETKCTLKSFTVEKGIYQTSNFRVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVNFNFNGLTGTGVLTESNKKFLPFQQFGRDIADTTDAVRDPQTLEILDITPCSFGGVSVITPGTNTSNQVAVLYQDVNCTEVPVAIHADQLTPTWRVYSTGSNVFQTRAGCLIGAEHVNNSYECDIPIGAGICASYQTQTNSPRRARSVASQSIIAYTMSLGAENSVAYSNNSIAIPTNFTISVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQLNRALTGIAVEQDKNTQEVFAQVKQIYKTPPIKDFGGFNFSQILPDPSKPSKRSFIEDLLFNKVTLADAGFIKQYGDCLGDIAARDLICAQKFNGLTVLPPLLTDEMIAQYTSALLAGTITSGWTFGAGAALQIPFAMQMAYRFNGIGVTQNVLYENQKLIANQFNSAIGKIQDSLSSTASALGKLQDVVNQNAQALNTLVKQLSSNFGAISSVLNDILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRASANLAATKMSECVLGQSKRVDFCGKGYHLMSFPQSAPHGVVFLHVTYVPAQEKNFTTAPAICHDGKAHFPREGVFVSNGTHWFVTQRNFYEPQIITTDNTFVSGNCDVVIGIVNNTVYDPLQPELDSFKEELDKYFKNHTSPDVDLGDISGINASVVNIQKEIDRLNEVAKNLNESLIDLQELGKYEQYIKWPWYIWLGFIAGLIAIVMVTIMLCCMTSCCSCLKGCCSCGSCCKFDEDDSEPVLKGVKLHYT*'}\n"
     ]
    }
   ],
   "source": [
    "# Read in fasta file\n",
    "\n",
    "fasta_filename = \"data/spikeprot0309.fasta\"\n",
    "fasta_samples = parse_fasta(fasta_filename)\n",
    "\n",
    "print(\"Number of FASTA entries: \", len(fasta_samples.keys()))\n",
    "print(\"\")\n",
    "print(\"Example FASTA entry: \")\n",
    "for (_, value) in fasta_samples.items():\n",
    "    print(value)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of metadata entries:  1225793\n",
      "\n",
      "Example metadata entry: \n",
      "{'Virus name': 'Australia/NT12/2020', 'Type': 'betacoronavirus', 'Accession ID': 'EPI_ISL_426900', 'Collection date': '2020', 'Location': 'Oceania / Australia / Northern territory', 'Additional location information': '', 'Sequence length': '29862', 'Host': 'Human', 'Patient age': 'unknown', 'Gender': 'unknown', 'Clade': 'G', 'Pango lineage': 'B.1', 'Pangolin version': '2021-04-21', 'Variant': '', 'AA Substitutions': '(NSP15_A283V,NSP12_P323L,Spike_D614G)', 'Submission date': '2020-04-17', 'Is reference?': '', 'Is complete?': 'True', 'Is high coverage?': 'True', 'Is low coverage?': '', 'N-Content': '0.00691236470311', 'GC-Content': '0.379674275888'}\n"
     ]
    }
   ],
   "source": [
    "# Read in metadata file\n",
    "\n",
    "metadata_filename = \"data/metadata.tsv\"\n",
    "metadata_samples = parse_gisaid_metadata(metadata_filename)\n",
    "\n",
    "print(\"Number of metadata entries: \", len(metadata_samples.keys()))\n",
    "print(\"\")\n",
    "print(\"Example metadata entry: \")\n",
    "for (_, value) in metadata_samples.items():\n",
    "    print(value)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of global timetree entries:  7354\n",
      "\n",
      "Example global timetree entry: \n",
      "{'strain': 'Wuhan/WH01/2019', 'path_length': '0.0020887245693756995', 'timetree_id': 0, 'clade_membership': '19A', 'num_date': '2019.9849315068493', 'num_date_CI': '{2019.9849315068493,2019.9849315068493}', 'subclade_membership': '19A', 'pango_lineage': 'B', 'GISAID_clade': 'L', 'location': 'Wuhan', 'division': 'Hubei', 'country': 'China', 'region': 'Asia', 'host': 'Human', 'age': '44', 'sex': 'Male', 'recency': 'Older', 'country_exposure': 'China', 'division_exposure': 'Hubei', 'div': '2'}\n"
     ]
    }
   ],
   "source": [
    "# Read in timetree file\n",
    "\n",
    "timetree_filename = \"data/nextstrain_ncov_global_timetree.nexus\"\n",
    "(timetree_samples, _) = parse_timetree(timetree_filename)\n",
    "\n",
    "print(\"Number of global timetree entries: \", len(timetree_samples.keys()))\n",
    "print(\"\")\n",
    "print(\"Example global timetree entry: \")\n",
    "for (_, value) in timetree_samples.items():\n",
    "    print(value)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching EPI_ISL ID's between GISAID metadata and FASTA:  702013\n"
     ]
    }
   ],
   "source": [
    "# Find matches between metadata and fasta data using EPI_ISL_ID\n",
    "\n",
    "metadata_fasta_matches = []\n",
    "for epi_isl in metadata_samples.keys():\n",
    "    if epi_isl in fasta_samples:\n",
    "        metadata_fasta_matches.append(epi_isl)\n",
    "        \n",
    "print(\"Number of matching EPI_ISL ID's between GISAID metadata and FASTA: \", len(metadata_fasta_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example matched sample: \n",
      "{'strain': 'Australia/NT12/2020', 'clade': 'G', 'sequence': 'MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIVNNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLEGKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSALEPLVDLPIGINITRFQTLLALHRSYLTPGDSSSGWTXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVNFNFNGLTGTGVLTESNKKFLPFQQFGRDIADTTDAVRDPQTLEILDITPCSFGGVSVITPGTNTSNQVAVLYQGVNCTEVPVAIHADQLTPTWRVYSTGSNVFQTRAGCLIGAEHVNNSYECDIPIGAGICASYQTQTNSPRRARSVASQSIIAYTMSLGAENSVAYSNNSIAIPTNFTISVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQLNRALTGIAVEQDKNTQEVFAQVKQIYKTPPIKDFGGFNFSQILPDPSKPSKRSFIEDLLFNKVTLADAGFIKQYGDCLGDIAARDLICAQKFNGLTVLPPLLTDEMIAQYTSALLAGTITSGWTFGAGAALQIPFAMQMAYRFNGIGVTQNVLYENQKLIANQFNSAIGKIQDSLSSTASALGKLQDVVNQNAQALNTLVKQLSSNFGAISSVLNDILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRASANLAATKMSECVLGQSKRVDFCGKGYHLMSFPQSAPHGVVFLHVTYVPAQEKNFTTAPAICHDGKAHFPREGVFVSNGTHWFVTQRNFYEPQIITTDNTFVSGNCDVVIGIVNNTVYDPLQPELDSFKEELDKYFKNHTSPDVDLGDISGINASVVNIQKEIDRLNEVAKNLNESLIDLQELGKYEQYIKWPWYIWLGFIAGLIAIVMVTIMLCCMTSCCSCLKGCCSCGSCCKFDEDDSEPVLKGVKLHYT*'}\n"
     ]
    }
   ],
   "source": [
    "# Build dictionary of entries that match between metadata and fasta\n",
    "\n",
    "metadata_fasta_samples = {}\n",
    "for epi_isl in metadata_fasta_matches:\n",
    "    fasta = fasta_samples[epi_isl]\n",
    "    metadata = metadata_samples[epi_isl]\n",
    "    \n",
    "    sample = {\n",
    "        \"strain\": metadata[\"Virus name\"],\n",
    "        \"clade\": metadata[\"Clade\"],\n",
    "        \"sequence\": fasta[\"Sequence\"],\n",
    "    }\n",
    "    \n",
    "    metadata_fasta_samples[epi_isl] = sample\n",
    "\n",
    "print(\"Example matched sample: \")\n",
    "for (_, value) in metadata_fasta_samples.items():\n",
    "    print(value)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find matches between timetree and fasta data using strain\n",
    "\n",
    "# timetree_fasta_matches = []\n",
    "# for epi_isl in metadata_fasta_matches:\n",
    "#     strain = metadata_samples[epi_isl][\"Strain\"]\n",
    "#     if strain in timetree_samples:\n",
    "#         timetree_fasta_matches.append(epi_isl)\n",
    "        \n",
    "# print(\"Number of matches between EPI_ISL ID's and NextStrain global timetree: \", len(timetree_fasta_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build dictionary of entries that match between all three files\n",
    "\n",
    "# matched_samples = {}\n",
    "# for epi_isl in timetree_fasta_matches:\n",
    "#     fasta = fasta_samples[epi_isl]\n",
    "#     metadata = metadata_samples[epi_isl]\n",
    "#     timetree = timetree_samples[metadata[\"Strain\"]]\n",
    "    \n",
    "#     sample = {\n",
    "#         \"strain\": metadata[\"Strain\"],\n",
    "#         \"divergence\": int(timetree[\"div\"]),\n",
    "#         \"clade\": timetree[\"clade_membership\"],\n",
    "#         \"subclade\": timetree[\"subclade_membership\"],\n",
    "#         \"sequence\": fasta[\"Sequence\"],\n",
    "#     }\n",
    "    \n",
    "#     matched_samples[epi_isl] = sample\n",
    "\n",
    "# print(\"Example matched sample: \")\n",
    "# for (_, value) in matched_samples.items():\n",
    "#     print(value)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amino mapping:  {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'U': 18, 'V': 19, 'W': 20, 'X': 21, 'Y': 22, 'Z': 23, '*': 24}\n",
      "Clade mapping:  {'G': 0, 'GH': 1, 'GR': 2, 'GRY': 3, 'GV': 4, 'L': 5, 'O': 6, 'S': 7, 'V': 8}\n"
     ]
    }
   ],
   "source": [
    "# Map from amino acids and clades to ints\n",
    "\n",
    "with open(\"data/amino_list.txt\", encoding=\"utf8\") as f:\n",
    "    amino_list = f.read().strip().split(',')\n",
    "amino_codes = {}\n",
    "for (i, v) in enumerate(amino_list):\n",
    "    amino_codes[v] = i\n",
    "    \n",
    "clade_list = sorted(list(set([value[\"clade\"] for (_, value) in metadata_fasta_samples.items()])))\n",
    "clade_codes = {}\n",
    "for (i, v) in enumerate(clade_list):\n",
    "    clade_codes[v] = i\n",
    "    \n",
    "print(\"Amino mapping: \", amino_codes)\n",
    "print(\"Clade mapping: \", clade_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training and validation data from string to numerical format\n",
    "    \n",
    "def amino_to_num(data_list, amino_codes):\n",
    "    new_data = []\n",
    "    \n",
    "    for seq in data_list:\n",
    "        new_seq = np.array([amino_codes[char] for char in seq])\n",
    "        new_data.append(new_seq)\n",
    "        \n",
    "    return np.array(new_data, dtype=np.object)\n",
    "\n",
    "def clade_to_num(data, clade_codes):\n",
    "    new_data = [clade_codes[clade] for clade in data]\n",
    "    return np.array(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training and validation datasets\n",
    "    \n",
    "train_data = []\n",
    "train_data_num = []\n",
    "train_label = []\n",
    "\n",
    "validation_data = []\n",
    "validation_data_num = []\n",
    "validation_label = []\n",
    "\n",
    "for (i, (_, value)) in enumerate(metadata_fasta_samples.items()):\n",
    "    if i % 10 == 0:\n",
    "        validation_data.append(value[\"sequence\"])\n",
    "        validation_label.append(value[\"clade\"])\n",
    "    else:\n",
    "        train_data.append(value[\"sequence\"])\n",
    "        train_label.append(value[\"clade\"])\n",
    "        \n",
    "train_data_num = amino_to_num(train_data, amino_codes)\n",
    "train_label_num = clade_to_num(train_label, clade_codes)\n",
    "\n",
    "validation_data_num = amino_to_num(validation_data, amino_codes)\n",
    "validation_label_num = clade_to_num(validation_label, clade_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:  MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIVNNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLEGKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSALEPLVDLPIGINITRFQTLLALHRSYLTPGDSSSGWXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPT\n",
      "\n",
      "Sample data numerical: \n",
      " [11  5 19  5 10 19 10 10 13 10 19 16 16 14  2 19 12 10 17 17 15 17 14 10\n",
      " 13 13  0 22 17 12 16  5 17 15  6 19 22 22 13  3  9 19  5 15 16 16 19 10\n",
      "  7 16 17 14  3 10  5 10 13  5  5 16 12 19 17 20  5  7  0  8  7 19 16  6\n",
      " 17 12  6 17  9 15  5  3 12 13 19 10 13  5 12  3  6 19 22  5  0 16 17  4\n",
      "  9 16 12  8  8 15  6 20  8  5  6 17 17 10  3 16  9 17 14 16 10 10  8 19\n",
      " 12 12  0 17 12 19 19  8  9 19  2  4  5 14  5  2 12  3 13  5 10  6 19 22\n",
      " 22  7  9 12 12  9 16 20 11  4 16  4  5 15 19 22 16 16  0 12 12  2 17  5\n",
      "  4 22 19 16 14 13  5 10 11  3 10  4  6  9 14  6 12  5  9 12 10 15  4  5\n",
      " 19  5  9 12  8  3  6 22  5  9  8 22 16  9  7 17 13  8 12 10 19 15  3 10\n",
      " 13 14  6  5 16  0 10  4 13 10 19  3 10 13  8  6  8 12  8 17 15  5 14 17\n",
      " 10 10  0 10  7 15 16 22 10 17 13  6  3 16 16 16  6 20 21 21 21 21 21 21\n",
      " 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21\n",
      " 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21 21\n",
      " 21 21 21 21 21 21 21 21 21 13 17  4 16  8 19 15  5 13 12  8 17 12 10  2\n",
      " 13  5  6  4 19  5 12  0 17 15  5  0 16 19 22  0 20 12 15  9 15  8 16 12\n",
      "  2 19  0  3 22 16 19 10 22 12 16  0 16  5 16 17  5  9  2 22  6 19 16 13\n",
      " 17  9 10 12  3 10  2  5 17 12 19 22  0  3 16  5 19  8 15  6  3  4 19 15\n",
      " 14  8  0 13  6 14 17  6  9  8  0  3 22 12 22  9 10 13  3  3  5 17  6  2\n",
      " 19  8  0 20 12 16 12 12 10  3 16  9 19  6  6 12 22 12 22 10 22 15 10  5\n",
      " 15  9 16 12 10  9 13  5  4 15  3  8 16 17  4  8 22 14  0  6 16 17 13  2\n",
      " 12  6 19  4  6  5 12  2 22  5 13 10 14 16 22  6  5 14 13 17]\n",
      "\n",
      "Sample label:  G\n",
      "Sample label numerical:  0\n"
     ]
    }
   ],
   "source": [
    "# Print sample data\n",
    "\n",
    "print(\"Sample data: \", train_data[0][:500])\n",
    "print(\"\\nSample data numerical: \\n\", train_data_num[0][:500])\n",
    "print(\"\\nSample label: \", train_label[0])\n",
    "print(\"Sample label numerical: \", train_label_num[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all data to numpy file\n",
    "\n",
    "# np.save(\"data/matched_samples.npy\", matched_samples, allow_pickle=True)\n",
    "# np.save(\"data/fasta_samples.npy\", fasta_samples, allow_pickle=True)\n",
    "# np.save(\"data/metadata_samples.npy\", metadata_samples, allow_pickle=True)\n",
    "# np.save(\"data/timetree_samples.npy\", timetree_samples, allow_pickle=True)\n",
    "\n",
    "np.save(\"data/amino_mapping.npy\", amino_list, allow_pickle=True)\n",
    "np.save(\"data/clade_mapping.npy\", clade_list, allow_pickle=True)\n",
    "\n",
    "np.save(\"data/train_data.npy\", train_data, allow_pickle=True)\n",
    "np.save(\"data/train_data_num.npy\", train_data_num, allow_pickle=True)\n",
    "np.save(\"data/train_label_clade.npy\", train_label, allow_pickle=True)\n",
    "np.save(\"data/train_label_clade_num.npy\", train_label_num, allow_pickle=True)\n",
    "\n",
    "np.save(\"data/validation_data.npy\", validation_data, allow_pickle=True)\n",
    "np.save(\"data/validation_data_num.npy\", validation_data_num, allow_pickle=True)\n",
    "np.save(\"data/validation_label_clade.npy\", validation_label, allow_pickle=True)\n",
    "np.save(\"data/validation_label_clade_num.npy\", validation_label_num, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
