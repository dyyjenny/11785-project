{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Use biopython library to process fasta files\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in fasta file\n",
    "\n",
    "fasta_filename = \"data/spikeprot0309.fasta\"\n",
    "fasta_entries = SeqIO.parse(fasta_filename, \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of FASTA entries:  702408\n",
      "\n",
      "Example FASTA entry: \n",
      "{'Strain': 'Wuhan/WIV04/2019', 'Submission Date': '2019-12-30', 'EPI_ISL': 'EPI_ISL_402124', 'Division of Exposure': 'hCoV-19^^Hubei', 'Originating Lab': 'Wuhan Jinyintan Hospital', 'Submitting Lab': 'Wuhan Institute of Virology', 'Author': 'Shi', 'Country of Exposure': 'China', 'Sequence': 'MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIVNNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLEGKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSALEPLVDLPIGINITRFQTLLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNENGTITDAVDCALDPLSETKCTLKSFTVEKGIYQTSNFRVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVNFNFNGLTGTGVLTESNKKFLPFQQFGRDIADTTDAVRDPQTLEILDITPCSFGGVSVITPGTNTSNQVAVLYQDVNCTEVPVAIHADQLTPTWRVYSTGSNVFQTRAGCLIGAEHVNNSYECDIPIGAGICASYQTQTNSPRRARSVASQSIIAYTMSLGAENSVAYSNNSIAIPTNFTISVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQLNRALTGIAVEQDKNTQEVFAQVKQIYKTPPIKDFGGFNFSQILPDPSKPSKRSFIEDLLFNKVTLADAGFIKQYGDCLGDIAARDLICAQKFNGLTVLPPLLTDEMIAQYTSALLAGTITSGWTFGAGAALQIPFAMQMAYRFNGIGVTQNVLYENQKLIANQFNSAIGKIQDSLSSTASALGKLQDVVNQNAQALNTLVKQLSSNFGAISSVLNDILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRASANLAATKMSECVLGQSKRVDFCGKGYHLMSFPQSAPHGVVFLHVTYVPAQEKNFTTAPAICHDGKAHFPREGVFVSNGTHWFVTQRNFYEPQIITTDNTFVSGNCDVVIGIVNNTVYDPLQPELDSFKEELDKYFKNHTSPDVDLGDISGINASVVNIQKEIDRLNEVAKNLNESLIDLQELGKYEQYIKWPWYIWLGFIAGLIAIVMVTIMLCCMTSCCSCLKGCCSCGSCCKFDEDDSEPVLKGVKLHYT*'}\n"
     ]
    }
   ],
   "source": [
    "# Process each fasta sample into a dictionary\n",
    "# The key is the EPI_ISL ID, the value is a dictionary with metadata and the protein sequence\n",
    "\n",
    "fasta_samples = {}\n",
    "for entry in fasta_entries:\n",
    "    metadata_list = entry.description.split(\"|\")\n",
    "    protein_sequence = str(entry.seq)\n",
    "    \n",
    "    # Account for rows with missing data\n",
    "    while len(metadata_list) < 11:\n",
    "        metadata_list.append(\"\")\n",
    "        \n",
    "    # Remove \"hCoV-19/\" prefix and remove spaces from fasta strain\n",
    "    strain = metadata_list[1]\n",
    "    if strain[:8] == \"hCoV-19/\":\n",
    "        strain = strain[8:]\n",
    "    strain = strain.replace(\" \", \"\")\n",
    "        \n",
    "    # Create dictionary from entry\n",
    "    sample = {\n",
    "        \"Strain\": strain,\n",
    "        \"Submission Date\": metadata_list[2],\n",
    "        \"EPI_ISL\": metadata_list[3],\n",
    "        \"Division of Exposure\": metadata_list[5],\n",
    "        \"Originating Lab\": metadata_list[7],\n",
    "        \"Submitting Lab\": metadata_list[8],\n",
    "        \"Author\": metadata_list[9],\n",
    "        \"Country of Exposure\": metadata_list[10],\n",
    "        \"Sequence\": protein_sequence,\n",
    "    }\n",
    "    \n",
    "    # Add sample to fasta_samples\n",
    "    epi_isl = metadata_list[3]\n",
    "    assert(epi_isl[:7] == \"EPI_ISL\")\n",
    "    fasta_samples[epi_isl] = sample\n",
    "    \n",
    "print(\"Number of FASTA entries: \", len(fasta_samples.keys()))\n",
    "print(\"\")\n",
    "print(\"Example FASTA entry: \")\n",
    "for (_, value) in fasta_samples.items():\n",
    "    print(value)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of metadata entries:  3860\n",
      "\n",
      "Example metadata entry: \n",
      "{'Strain': 'Wuhan/WH01/2019', 'GISAID Clade': 'L', 'S1 mutations': '', 'Age': '44', 'Clade': '19A', 'Country': 'China', 'Country of Exposure': 'China', 'Admin Division': 'Hubei', 'Division of Exposure': 'Hubei', 'genbank_accession': 'LR757998.1', 'gisaid_epi_isl': 'EPI_ISL_406798', 'Host': 'Human', 'Location': 'Wuhan', 'Originating Lab': \"General Hospital of Central Theater Command of People's Liberation Army of China\", 'PANGO Lineage': 'B', 'Submission Date': 'Older', 'Region': 'Asia', 'Sex': 'Male', 'Emerging Clade': '19A', 'Submitting Lab': \"BGI & Institute of Microbiology, Chinese Academy of Sciences & Shandong First Medical University & Shandong Academy of Medical Sciences & General Hospital of Central Theater Command of People's Liberation Army of China\", 'url': '', 'Collection Data': '2019-12-26', 'Author': 'Weijun Chen et al (https://dx.doi.org/10.1016/S0140-6736(20)30251-8)', 'Region of Exposure': ''}\n"
     ]
    }
   ],
   "source": [
    "# Process nextstrain global metadata into a dictionary\n",
    "\n",
    "metadata_filename = \"data/nextstrain_ncov_global_metadata.tsv\"\n",
    "metadata_samples = {}\n",
    "with open(metadata_filename, \"r\") as f:\n",
    "    tsv_reader = csv.reader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    header = next(tsv_reader)\n",
    "    \n",
    "    for row in tsv_reader:\n",
    "        # Create dictionary from row\n",
    "        sample = {}\n",
    "        for i in range(len(row)):\n",
    "            sample[header[i]] = row[i]\n",
    "            \n",
    "        # Add sample to metadata_samples\n",
    "        epi_isl = row[10]\n",
    "        assert(epi_isl[:7] == \"EPI_ISL\")\n",
    "        metadata_samples[epi_isl] = sample\n",
    "        \n",
    "    print(\"Number of metadata entries: \", len(metadata_samples.keys()))\n",
    "    print(\"\")\n",
    "    print(\"Example metadata entry: \")\n",
    "    for (_, value) in metadata_samples.items():\n",
    "        print(value)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching EPI_ISL ID's between NextStrain metadata and FASTA:  2488\n"
     ]
    }
   ],
   "source": [
    "# Find matches between metadata and fasta\n",
    "\n",
    "metadata_fasta_matches = []\n",
    "for epi_isl in metadata_samples.keys():\n",
    "    if epi_isl in fasta_samples:\n",
    "        metadata_fasta_matches.append(epi_isl)\n",
    "        \n",
    "print(\"Number of matching EPI_ISL ID's between NextStrain metadata and FASTA: \", len(metadata_fasta_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in nextstrain global timetree file\n",
    "\n",
    "timetree_filename = \"data/nextstrain_ncov_global_timetree.nexus\"\n",
    "with open(timetree_filename, \"r\") as f:\n",
    "    timetree_str = f.read()\n",
    "    timetree_str = timetree_str[33:-6] # Remove start and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Separate nexstrain global timetree data into a tree structure, and an array of individual entries\n",
    "\n",
    "tree_structure = []\n",
    "timetree_entries = []\n",
    "entry_start = 0\n",
    "\n",
    "in_strain_name = False\n",
    "in_description = False\n",
    "in_tree_length = False\n",
    "\n",
    "for (i, char) in enumerate(timetree_str):\n",
    "    # Advance parser state, keeping track of when each tree entry starts and ends\n",
    "    # Parser states advances from none -> in_strain_name -> in_description -> in_tree_length\n",
    "    # Separately keep track of the tree structure, using each entry's ID in timetree_entries\n",
    "    if in_strain_name:\n",
    "        if char == '[':\n",
    "            in_strain_name = False\n",
    "            in_description = True\n",
    "            \n",
    "    elif in_description:\n",
    "        if char == ']':\n",
    "            in_description = False\n",
    "            in_tree_length = True\n",
    "            \n",
    "    elif in_tree_length:\n",
    "        if not char.isdigit() and char != ':' and char != '.':\n",
    "            in_tree_length = False\n",
    "            timetree_entries.append(timetree_str[entry_start:i])\n",
    "            \n",
    "            tree_structure.append(str(len(timetree_entries)))\n",
    "            tree_structure.append(char)\n",
    "            \n",
    "    else:\n",
    "        if char != ',' and char != '(' and char != ')':\n",
    "            in_strain_name = True\n",
    "            entry_start = i\n",
    "        else:\n",
    "            tree_structure.append(char)\n",
    "            \n",
    "tree_structure = \"\".join(tree_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of global timetree entries:  7354\n",
      "\n",
      "Example global timetree entry: \n",
      "{'strain': 'Wuhan/WH01/2019', 'path_length': '0.0020887245693756995', 'timetree_id': 0, 'clade_membership': '19A', 'num_date': '2019.9849315068493', 'num_date_CI': '{2019.9849315068493,2019.9849315068493}', 'subclade_membership': '19A', 'pango_lineage': 'B', 'GISAID_clade': 'L', 'location': 'Wuhan', 'division': 'Hubei', 'country': 'China', 'region': 'Asia', 'host': 'Human', 'age': '44', 'sex': 'Male', 'recency': 'Older', 'country_exposure': 'China', 'division_exposure': 'Hubei', 'div': '2'}\n"
     ]
    }
   ],
   "source": [
    "# Parse timetree entries into a dictionary\n",
    "\n",
    "timetree_samples = {}\n",
    "for (timetree_id, entry) in enumerate(timetree_entries):\n",
    "    (strain, entry) = entry.split(\"[&\")\n",
    "    (entry, path_length) = entry.split(\"]:\")\n",
    "    tokens = entry.split(\",\")\n",
    "    \n",
    "    # Create dictionary from entry\n",
    "    sample = {\n",
    "        \"strain\": strain,\n",
    "        \"path_length\": path_length,\n",
    "        \"timetree_id\": timetree_id,\n",
    "    }\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            (name, value) = token.split(\"=\")\n",
    "            sample[name] = value\n",
    "        except ValueError:\n",
    "            # Handle comma within num_date_CI\n",
    "            sample[\"num_date_CI\"] += \",\" + token\n",
    "            \n",
    "    # Add sample to timetree_samples\n",
    "    timetree_samples[strain] = sample\n",
    "\n",
    "print(\"Number of global timetree entries: \", len(timetree_entries))\n",
    "print(\"\")\n",
    "print(\"Example global timetree entry: \")\n",
    "for (_, value) in timetree_samples.items():\n",
    "    print(value)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches between EPI_ISL ID's and NextStrain global timetree:  683\n"
     ]
    }
   ],
   "source": [
    "# Find matches between valid EPI_ISL IDs and global timetree data\n",
    "\n",
    "matches = []\n",
    "for epi_isl in metadata_fasta_matches:\n",
    "    strain = metadata_samples[epi_isl][\"Strain\"]\n",
    "    if strain in timetree_samples:\n",
    "        matches.append(epi_isl)\n",
    "        \n",
    "print(\"Number of matches between EPI_ISL ID's and NextStrain global timetree: \", len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example matched sample: \n",
      "{'strain': 'Wuhan/WH01/2019', 'divergence': 2, 'clade': '19A', 'subclade': '19A', 'sequence': 'MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIVNNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLEGKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSALEPLVDLPIGINITRFQTLLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNENGTITDAVDCALDPLSETKCTLKSFTVEKGIYQTSNFRVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVNFNFNGLTGTGVLTESNKKFLPFQQFGRDIADTTDAVRDPQTLEILDITPCSFGGVSVITPGTNTSNQVAVLYQDVNCTEVPVAIHADQLTPTWRVYSTGSNVFQTRAGCLIGAEHVNNSYECDIPIGAGICASYQTQTNSPRRARSVASQSIIAYTMSLGAENSVAYSNNSIAIPTNFTISVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQLNRALTGIAVEQDKNTQEVFAQVKQIYKTPPIKDFGGFNFSQILPDPSKPSKRSFIEDLLFNKVTLADAGFIKQYGDCLGDIAARDLICAQKFNGLTVLPPLLTDEMIAQYTSALLAGTITSGWTFGAGAALQIPFAMQMAYRFNGIGVTQNVLYENQKLIANQFNSAIGKIQDSLSSTASALGKLQDVVNQNAQALNTLVKQLSSNFGAISSVLNDILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRASANLAATKMSECVLGQSKRVDFCGKGYHLMSFPQSAPHGVVFLHVTYVPAQEKNFTTAPAICHDGKAHFPREGVFVSNGTHWFVTQRNFYEPQIITTDNTFVSGNCDVVIGIVNNTVYDPLQPELDSFKEELDKYFKNHTSPDVDLGDISGINASVVNIQKEIDRLNEVAKNLNESLIDLQELGKYEQYIKWPWYIWLGFIAGLIAIVMVTIMLCCMTSCCSCLKGCCSCGSCCKFDEDDSEPVLKGVKLHYT*'}\n"
     ]
    }
   ],
   "source": [
    "# Build dictionary of entries that match between all files\n",
    "\n",
    "matched_samples = {}\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for epi_isl in matches:\n",
    "    fasta = fasta_samples[epi_isl]\n",
    "    metadata = metadata_samples[epi_isl]\n",
    "    timetree = timetree_samples[metadata[\"Strain\"]]\n",
    "    \n",
    "    sample = {\n",
    "        \"strain\": metadata[\"Strain\"],\n",
    "        \"divergence\": int(timetree[\"div\"]),\n",
    "        \"clade\": timetree[\"clade_membership\"],\n",
    "        \"subclade\": timetree[\"subclade_membership\"],\n",
    "        \"sequence\": fasta[\"Sequence\"],\n",
    "    }\n",
    "    \n",
    "    matched_samples[epi_isl] = sample\n",
    "\n",
    "print(\"Example matched sample: \")\n",
    "for (_, value) in matched_samples.items():\n",
    "    print(value)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map from amino acids and clades to ints\n",
    "\n",
    "with open(\"data/amino_list.txt\", encoding=\"utf8\") as f:\n",
    "    amino_list = f.read().strip().split(',')\n",
    "amino_codes = {}\n",
    "for (i, v) in enumerate(amino_list):\n",
    "    amino_codes[v] = i\n",
    "    \n",
    "clade_list = list(set([value[\"clade\"] for (_, value) in matched_samples.items()]))\n",
    "clade_codes = {}\n",
    "for (i, v) in enumerate(clade_list):\n",
    "    clade_codes[v] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training and validation data from string to numerical format\n",
    "    \n",
    "def amino_to_num(data_list, amino_codes):\n",
    "    new_data = []\n",
    "    \n",
    "    for seq in data_list:\n",
    "        new_seq = np.array([amino_codes[char] for char in seq])\n",
    "        new_data.append(new_seq)\n",
    "        \n",
    "    return np.array(new_data, dtype=np.object)\n",
    "\n",
    "def clade_to_num(data, clade_codes):\n",
    "    new_data = [clade_codes[clade] for clade in data]\n",
    "    return np.array(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training and validation datasets\n",
    "    \n",
    "train_data = []\n",
    "train_data_numerical = []\n",
    "train_label = []\n",
    "\n",
    "validation_data = []\n",
    "validation_data_numerical = []\n",
    "validation_label = []\n",
    "\n",
    "for (i, (_, value)) in enumerate(matched_samples.items()):\n",
    "    if i % 10 == 0:\n",
    "        validation_data.append(value[\"sequence\"])\n",
    "        validation_label.append(value[\"clade\"])\n",
    "    else:\n",
    "        train_data.append(value[\"sequence\"])\n",
    "        train_label.append(value[\"clade\"])\n",
    "        \n",
    "train_data_numerical = amino_to_num(train_data, amino_codes)\n",
    "train_label_numerical = clade_to_num(train_label, clade_codes)\n",
    "\n",
    "validation_data_numerical = amino_to_num(validation_data, amino_codes)\n",
    "validation_label_numerical = clade_to_num(validation_label, clade_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all to numpy file\n",
    "\n",
    "np.save(\"data/matched_samples.npy\", matched_samples, allow_pickle=True)\n",
    "np.save(\"data/fasta_samples.npy\", fasta_samples, allow_pickle=True)\n",
    "np.save(\"data/metadata_samples.npy\", metadata_samples, allow_pickle=True)\n",
    "np.save(\"data/timetree_samples.npy\", timetree_samples, allow_pickle=True)\n",
    "\n",
    "np.save(\"data/amino_mapping.npy\", amino_list, allow_pickle=True)\n",
    "np.save(\"data/clade_mapping.npy\", clade_list, allow_pickle=True)\n",
    "\n",
    "np.save(\"data/train_data.npy\", train_data, allow_pickle=True)\n",
    "np.save(\"data/train_data_numerical.npy\", train_data_numerical, allow_pickle=True)\n",
    "np.save(\"data/train_label.npy\", train_label, allow_pickle=True)\n",
    "np.save(\"data/train_label_numerical.npy\", train_label_numerical, allow_pickle=True)\n",
    "\n",
    "np.save(\"data/validation_data.npy\", validation_data, allow_pickle=True)\n",
    "np.save(\"data/validation_data_numerical.npy\", validation_data_numerical, allow_pickle=True)\n",
    "np.save(\"data/validation_label.npy\", validation_label, allow_pickle=True)\n",
    "np.save(\"data/validation_label_numerical.npy\", validation_label_numerical, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
