{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "bert_hydrophilicity_embedding_generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94oCKclTNUmB"
      },
      "source": [
        "Generate Bert Embedding + Hydrophilicity Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPzDslK3wfCJ"
      },
      "source": [
        "!pip install transformers\n",
        "! pip install tape_proteins"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-zAfZZPmkGA"
      },
      "source": [
        "import numpy as np\n",
        "from torch import Generator\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from tempfile import TemporaryFile\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Preliminaries\n",
        "\n",
        "# from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
        "\n",
        "# Models\n",
        "\n",
        "import torch.nn as nn\n",
        "# from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Training\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "# from tape import ProteinBertModel, TAPETokenizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DomvhhHVYbag",
        "outputId": "aad2ab4d-bd2b-42e2-e03e-a2ffac9489fe"
      },
      "source": [
        "# Direct to data folder\n",
        "%cd /content/drive/MyDrive/project_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1g3rUaoGdVQ9MqEkCjpoH7he2HFJB6Wws/project_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egPwn7uXbyhf"
      },
      "source": [
        "# Hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf9PLbieb5Mr"
      },
      "source": [
        "hyperparams = {\n",
        "    'validation_split' : 0.2,\n",
        "    'split_seed' : 42,\n",
        "    'batch_size' : 32,\n",
        "    'lr' : 5e-3,\n",
        "    'weight_decay' : 5e-6,\n",
        "    'class_num' : 9, # need to change\n",
        "    'dropout_prob' : 0.2,\n",
        "    'rnn_hidden_size' : 256,\n",
        "    'rnn_layer_num' : 3,\n",
        "    'soft_aug_size' : 40\n",
        "    }\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "alpha = [np.random.beta(8,8) for _ in range(hyperparams['soft_aug_size'])]\n",
        "\n",
        "modes = ('train', 'dev', 'soft')\n",
        "\n",
        "vocab = {'A': 1.8, 'R':-4.5,'N':-3.5,'D':-3.5,'C':2.5,'Q':-3.5,'E':-3.5,'G':-0.4,'H' :-3.2,'I':4.5,\n",
        "         'L':3.8,'K':-3.9,'M':1.9,'F':2.8,'P':-1.6,'S':-0.8,'T':-0.7,'W':-0.9,'X':2, 'Y':-1.3,'V':4.2,'*':-100}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KcXWZ_eaaFS"
      },
      "source": [
        "# Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6xx2cavnAdX"
      },
      "source": [
        "train_data = np.load(\"train_embeddings.npy\", allow_pickle=True)\n",
        "# divergence\n",
        "train_labels = raw_train_labels = np.load('train_label_balanced_new.npy', allow_pickle=True)\n",
        "# clade\n",
        "# train_labels_clade = np.load('train_label_clade_num.npy', allow_pickle=True)\n",
        "\n",
        "val_data = np.load(\"valid_embeddings.npy\", allow_pickle=True)\n",
        "# divergence\n",
        "val_labels = raw_val_labels = np.load(\"validation_label_balanced_new.npy\", allow_pickle=True)\n",
        "# clade\n",
        "# val_labels_clade = np.load('validation_label_clade_num.npy', allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeW5JAV7rnqx",
        "outputId": "f0b8fb8c-b137-411e-a6d0-5a3c56dce0c1"
      },
      "source": [
        "print(train_data.shape)\n",
        "# print(np.shape(train_labels_clade))\n",
        "print(val_data.shape)\n",
        "# print(np.shape(val_labels_clade))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(43434,)\n",
            "(4842,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TfG4VgSH9k2"
      },
      "source": [
        "## mapping for DIVERGENCE labels in training data\n",
        "* no need to run for clade"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwBgK-N_jH0F"
      },
      "source": [
        "# Importing the relevant modules\n",
        "from tape import ProteinBertModel, TAPETokenizer\n",
        "model = ProteinBertModel.from_pretrained('bert-base')\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "tokenizer = TAPETokenizer(vocab='iupac')  # iupac is the vocab for TAPE models, use unirep for the UniRep model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWYquHZpi-rJ"
      },
      "source": [
        "# Bert Embedding\n",
        "train_embeds = []\n",
        "for i, x in enumerate(data_loader['train']):\n",
        "    if i > 0: break\n",
        "    for text in x:\n",
        "        token_id = tokenizer.encode(text)\n",
        "        token_id = np.pad(token_id, (0, (1280 - len(token_id))))\n",
        "        token_id = token_id[:1280]\n",
        "\n",
        "        tokens.append(torch.tensor(token_id).reshape(1,-1))\n",
        "    token_ids = torch.cat(tokens)\n",
        "    token_ids = token_ids.to(device)\n",
        "    output = model(token_ids)\n",
        "    word_embedding = torch.mean(output[0], dim=1).reshape(-1, 1)\n",
        "    train_embeds.append(word_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5KaydS7xFo_"
      },
      "source": [
        "train_embeds = [embed.cpu().numpy() for embed in train_embeds]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5dorpJqxPix"
      },
      "source": [
        "train_embeds = np.array(train_embeds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWjByl-OxrdY"
      },
      "source": [
        "np.save('train_embed.npy', train_embeds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuMtsvSplfys"
      },
      "source": [
        "# Hydrophilicity Encoding\n",
        "vocab = {'A': 1.8, 'R':-4.5,'N':-3.5,'D':-3.5,'C':2.5,'Q':-3.5,'E':-3.5,'G':-0.4,'H' :-3.2,'I':4.5,\n",
        "         'L':3.8,'K':-3.9,'M':1.9,'F':2.8,'P':-1.6,'S':-0.8,'T':-0.7,'W':-0.9,'X':2, 'Y':-1.3,'V':4.2,'*':-100}\n",
        "\n",
        "train_encoders = []\n",
        "for text in train_data:\n",
        "    map = [vocab[x] for x in text]\n",
        "    map = map + [0] * (1280-len(map))\n",
        "    map = map[:1280]\n",
        "    x = torch.FloatTensor(map).to(device)\n",
        "    x = torch.unsqueeze(x,dim=1)\n",
        "    train_encoders.append(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJtdUqFWx5x5"
      },
      "source": [
        "train_encoders = np.array([encode.cpu().numpy() for encode in train_encoders])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kV0SbA00yUx5"
      },
      "source": [
        "train_d = np.concatenate((train_embeds, train_encoders), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzxwZV95thAR"
      },
      "source": [
        "val_embed = []\n",
        "for text in val_data:\n",
        "    token_ids = torch.tensor([tokenizer.encode(text[:-1])])\n",
        "    token_ids = token_ids.to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(token_ids)\n",
        "    word_embedding = torch.mean(output[0], dim=1).reshape(-1, 1)\n",
        "    x = word_embedding\n",
        "    x = torch.squeeze(x, dim=1)\n",
        "    # print(x.shape)\n",
        "    val_embed.append(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S4avJA1IBed"
      },
      "source": [
        "## data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWLgD1QTZ-jl"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, X, Y = None, test=False, augment=None, aug_size=None):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.test = test\n",
        "        self.augment = augment\n",
        "        self.aug_size = aug_size\n",
        "        if augment == \"random_replace\":\n",
        "            self._random_replace()\n",
        "\n",
        "    def __len__(self):\n",
        "        assert(len(self.X) == len(self.Y))\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        x = self.X[idx]\n",
        "        if not self.test:\n",
        "            y = self.Y[idx]\n",
        "        else:\n",
        "            y = -1\n",
        "        return (x, y)\n",
        "\n",
        "    def _random_replace(self):\n",
        "        ori_len = self.X.shape[0]\n",
        "        aug_idx = np.random.choice(range(ori_len), size=self.aug_size,replace=False)\n",
        "        aug_set = [self.X[i] for i in aug_idx]\n",
        "        aug_data = []\n",
        "        aug_labels = []\n",
        "        for i, data in enumerate(aug_set):\n",
        "            j = np.random.choice(range(data.shape[0]))\n",
        "            data[j] = np.random.choice(range(24))\n",
        "            aug_data.append(data)\n",
        "            aug_labels.append(self.Y[aug_idx[i]])\n",
        "        self.X = np.append(self.X, np.array(aug_data, dtype='O'), axis=0)\n",
        "        self.Y = np.append(self.Y, np.array(aug_labels, dtype='O'), axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL0a_rnmEmQ6"
      },
      "source": [
        "train_dataset = Dataset(train_d, train_labels)\n",
        "val_dataset = Dataset(val_d, val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Qn4hBeAbp5S"
      },
      "source": [
        "datasets = {'train': train_dataset, \"dev\": val_dataset}\n",
        "\n",
        "data_loader = {\n",
        "    mode: DataLoader(\n",
        "        dataset = datasets[mode],\n",
        "        batch_size = 64,\n",
        "        shuffle = (mode == 'train' or mode == 'soft'),\n",
        "        # collate_fn = mix_pad_collate if mode == 'soft' else pad_collate\n",
        "    )\n",
        "    for mode in ['train', \"dev\"]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CL0_ts1lte0"
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),  \n",
        "            # nn.Dropout(0.2),\n",
        "            nn.ReLU(),\n",
        "            # nn.BatchNorm1d(1024),\n",
        "            nn.Linear(1024, 512), \n",
        "            # nn.Dropout(0.2),\n",
        "            nn.ReLU(),\n",
        "            # nn.BatchNorm1d(512),\n",
        "            nn.Linear(512, 256),  \n",
        "            # nn.Dropout(0.2),\n",
        "            nn.ReLU(),\n",
        "            # nn.BatchNorm1d(256),\n",
        "            nn.Linear(256, 9),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # print(x.shape)\n",
        "        output = self.out(x)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}