{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "model_2cnn_3fc_balancedData.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-KcXWZ_eaaFS"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpspj18Il05s",
        "outputId": "8d91c702-30a7-4a63-a184-4bc0405452da"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-zAfZZPmkGA"
      },
      "source": [
        "import numpy as np\n",
        "from torch import Generator\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from tempfile import TemporaryFile\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DomvhhHVYbag",
        "outputId": "06961010-0ad3-4a55-c1b3-91cae4423b33"
      },
      "source": [
        "%cd /content/drive/MyDrive/project_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1g3rUaoGdVQ9MqEkCjpoH7he2HFJB6Wws/project_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egPwn7uXbyhf"
      },
      "source": [
        "# Hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf9PLbieb5Mr"
      },
      "source": [
        "hyperparams = {\n",
        "    'validation_split' : 0.2,\n",
        "    'split_seed' : 42,\n",
        "    'batch_size' : 128,\n",
        "    'lr' : 2e-3,\n",
        "    'weight_decay' : 5e-6,\n",
        "    'class_num' : 11, # need to change\n",
        "    'dropout_prob' : 0.2,\n",
        "    'rnn_hidden_size' : 256,\n",
        "    'rnn_layer_num' : 3,\n",
        "    'soft_aug_size' : 40\n",
        "    }\n",
        "\n",
        "alpha = [np.random.beta(8,8) for _ in range(hyperparams['soft_aug_size'])]\n",
        "\n",
        "modes = ('train', 'dev', 'soft')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KcXWZ_eaaFS"
      },
      "source": [
        "# Old Data (unbalanced)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6xx2cavnAdX"
      },
      "source": [
        "train_data = np.load(\"train_data_num.npy\", allow_pickle=True)\n",
        "# clade\n",
        "train_labels_clade = np.load('train_label_clade_num.npy', allow_pickle=True)\n",
        "# pango\n",
        "train_labels_pango = np.load('train_label_pango_num.npy',allow_pickle=True)\n",
        "\n",
        "val_data = np.load(\"validation_data_num.npy\", allow_pickle=True)\n",
        "# clade\n",
        "val_labels_clade = np.load('validation_label_clade_num.npy', allow_pickle=True)\n",
        "# pango\n",
        "val_labels_pango = np.load('validation_label_pango_num.npy',allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abxJvHINmh__"
      },
      "source": [
        "# just taking 2/3 of the training data\n",
        "train_data = train_data[:len(train_data)*2//3]\n",
        "train_labels_clade = train_labels_clade[:len(train_labels_clade)*2//3]\n",
        "assert(len(train_data) == len(train_labels_clade))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN8clNgBsGr2",
        "outputId": "3878273f-0aee-4eba-af4e-35259aec0817"
      },
      "source": [
        "# run for CLADE\n",
        "assert(set(np.unique(train_labels_clade)) == set(np.unique(val_labels_clade)))\n",
        "hyperparams['class_num'] = len(np.unique(val_labels_clade))\n",
        "hyperparams['class_num']\n",
        "np.unique(val_labels_clade)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeW5JAV7rnqx",
        "outputId": "5c3ec49b-e620-463c-8534-5c6f6ce9b309"
      },
      "source": [
        "print(train_data.shape)\n",
        "print(np.shape(train_labels_clade))\n",
        "print(val_data.shape)\n",
        "print(np.shape(val_labels_clade))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(420522,)\n",
            "(420522,)\n",
            "(70088,)\n",
            "(70088,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psHCYFIQnRcW"
      },
      "source": [
        "# Balanced Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V37_NH3onTHt"
      },
      "source": [
        "train_data = np.load(\"train_data_balanced.npy\", allow_pickle=True)\n",
        "# clade\n",
        "train_labels_clade = np.load('train_label_balanced.npy', allow_pickle=True)\n",
        "\n",
        "\n",
        "val_data = np.load(\"validation_data_balanced.npy\", allow_pickle=True)\n",
        "# clade\n",
        "val_labels_clade = np.load('validation_label_balanced.npy', allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9y2YZKmnTHu",
        "outputId": "c1ba0875-512a-4b16-8fe9-03757ef32011"
      },
      "source": [
        "# run for CLADE\n",
        "assert(set(np.unique(train_labels_clade)) == set(np.unique(val_labels_clade)))\n",
        "hyperparams['class_num'] = len(np.unique(val_labels_clade))\n",
        "hyperparams['class_num']\n",
        "np.unique(val_labels_clade)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vhsoGlJnTHv",
        "outputId": "67f2b25b-f887-4ada-eb0a-fea72977c406"
      },
      "source": [
        "print(train_data.shape)\n",
        "print(np.shape(train_labels_clade))\n",
        "print(val_data.shape)\n",
        "print(np.shape(val_labels_clade))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(43434,)\n",
            "(43434,)\n",
            "(4842,)\n",
            "(4842,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksWh9oNThHKB",
        "outputId": "98b8d9af-c637-4e90-c32e-bcba805d00b0"
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([11,  5, 19, ..., 22, 17, 24])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDnxkbYAgxEF"
      },
      "source": [
        "# BERT embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybLKr0NAgyu0"
      },
      "source": [
        "train_data = np.load(\"train_embeddings.npy\", allow_pickle=True)\n",
        "# clade\n",
        "train_labels_clade = np.load('train_label_balanced.npy', allow_pickle=True)\n",
        "\n",
        "\n",
        "val_data = np.load(\"valid_embeddings.npy\", allow_pickle=True)\n",
        "# clade\n",
        "val_labels_clade = np.load('validation_label_balanced.npy', allow_pickle=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISWAKhXugyu2",
        "outputId": "b1eda213-e94c-4854-d9ae-d52d0f878fb2"
      },
      "source": [
        "# run for CLADE\n",
        "print(set(np.unique(train_labels_clade)))\n",
        "print(set(np.unique(val_labels_clade)))\n",
        "assert(set(np.unique(train_labels_clade)) == set(np.unique(val_labels_clade)))\n",
        "hyperparams['class_num'] = len(np.unique(val_labels_clade))\n",
        "hyperparams['class_num']\n",
        "np.unique(val_labels_clade)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0, 1, 2, 3, 4, 5, 6, 7, 8}\n",
            "{0, 1, 2, 3, 4, 5, 6, 7, 8}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DU9qtfLzgyu3",
        "outputId": "f3e6f56c-bff1-42a5-b589-b013759c3688"
      },
      "source": [
        "print(train_data.shape)\n",
        "print(np.shape(train_labels_clade))\n",
        "print(val_data.shape)\n",
        "print(np.shape(val_labels_clade))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(43434, 2048)\n",
            "(43434,)\n",
            "(4842, 2048)\n",
            "(4842,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wtuve7_g8F1",
        "outputId": "c1374013-0a41-440c-880e-ce494debe9b1"
      },
      "source": [
        "print(train_data[0].dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "float32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S4avJA1IBed"
      },
      "source": [
        "# data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWLgD1QTZ-jl"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, X, Y = None, test=False, augment=None, aug_size=None):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.test = test\n",
        "        self.augment = augment\n",
        "        self.aug_size = aug_size\n",
        "        if augment == \"random_replace\":\n",
        "            self._random_replace()\n",
        "\n",
        "    def __len__(self):\n",
        "        assert(len(self.X) == len(self.Y))\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        x = torch.from_numpy(self.X[idx].astype(float)).float()\n",
        "        x = torch.unsqueeze(x,dim=1)\n",
        "        if not self.test:\n",
        "            y = self.Y[idx]\n",
        "        else:\n",
        "            y = -1\n",
        "        return (x,y)\n",
        "\n",
        "    def _random_replace(self):\n",
        "        ori_len = self.X.shape[0]\n",
        "        aug_idx = np.random.choice(range(ori_len), size=self.aug_size,replace=False)\n",
        "        aug_set = [self.X[i] for i in aug_idx]\n",
        "        aug_data = []\n",
        "        aug_labels = []\n",
        "        for i, data in enumerate(aug_set):\n",
        "            j = np.random.choice(range(data.shape[0]))\n",
        "            data[j] = np.random.choice(range(24))\n",
        "            aug_data.append(data)\n",
        "            aug_labels.append(self.Y[aug_idx[i]])\n",
        "        self.X = np.append(self.X, np.array(aug_data, dtype='O'), axis=0)\n",
        "        self.Y = np.append(self.Y, np.array(aug_labels, dtype='O'), axis=0)\n",
        "\n",
        "\n",
        "def pad_collate(batch):\n",
        "    (xx,y) = zip(*batch)\n",
        "\n",
        "    xx_pad = pad_sequence(xx,padding_value=0) \n",
        "\n",
        "    x_lens = torch.LongTensor([len(x) for x in xx])\n",
        "    return (xx_pad, x_lens, torch.tensor(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHmy3n0jeP8Q"
      },
      "source": [
        "class SoftDataset(Dataset):\n",
        "    def __init__(self, X_a, X_b, Y):\n",
        "        self.X_a = X_a\n",
        "        self.X_b = X_b\n",
        "        self.Y = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        assert(len(self.X_a) == len(self.Y))\n",
        "        return len(self.Y)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        x_a = torch.Tensor(self.X_a[idx]).float()\n",
        "        x_a = torch.unsqueeze(x_a,dim=1)\n",
        "        x_b = torch.Tensor(self.X_b[idx]).float()\n",
        "        x_b = torch.unsqueeze(x_b,dim=1)\n",
        "        y = self.Y[idx]\n",
        "\n",
        "        return (x_a, x_b, y, idx)\n",
        "\n",
        "def mix_pad_collate(batch):\n",
        "    (xx_a, xx_b ,y, idx) = zip(*batch)\n",
        "\n",
        "    xx_pad_a = pad_sequence(xx_a,padding_value=0) \n",
        "    xx_pad_b = pad_sequence(xx_b,padding_value=0) \n",
        "\n",
        "    # y = pad_sequence(y,padding_value=0)\n",
        "\n",
        "    x_lens_a = torch.LongTensor([len(x) for x in xx_a])\n",
        "    x_lens_b = torch.LongTensor([len(x) for x in xx_b])\n",
        "    return (xx_pad_a, x_lens_a, xx_pad_b, x_lens_b, torch.tensor(y), idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rusk8ateGEy"
      },
      "source": [
        "def generate_mix_data(train_data, train_labels, aug_size):\n",
        "    num_aug = 0\n",
        "    pairs = []\n",
        "    input_a = []\n",
        "    input_b = []\n",
        "    labels = []\n",
        "\n",
        "    while(num_aug < aug_size):\n",
        "        i, j = np.random.randint(0, len(train_data)), np.random.randint(0, len(train_data))\n",
        "        if i != j and (i, j) not in pairs:\n",
        "            _a = alpha[num_aug]\n",
        "            pairs.append((i, j))\n",
        "            input_a.append(train_data[i])\n",
        "            input_b.append(train_data[j])\n",
        "            one_hot_a = np.zeros(len(np.unique(train_labels)), dtype=float)\n",
        "            one_hot_b = np.zeros(len(np.unique(train_labels)), dtype=float)\n",
        "            one_hot_a[train_labels[i]] = 1\n",
        "            one_hot_b[train_labels[j]] = 1\n",
        "\n",
        "            one_hot = list(_a * one_hot_a + (1-_a) * one_hot_b)\n",
        "            labels.append(one_hot)\n",
        "\n",
        "            num_aug += 1\n",
        "\n",
        "    return SoftDataset(input_a, input_b, labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgyMR6qGy8MV"
      },
      "source": [
        "soft_dataset = generate_mix_data(train_data, train_labels_clade, hyperparams['soft_aug_size'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL0a_rnmEmQ6"
      },
      "source": [
        "train_dataset = Dataset(train_data, train_labels_clade, augment=\"random_replace\", aug_size=40)\n",
        "val_dataset = Dataset(val_data, val_labels_clade)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Qn4hBeAbp5S"
      },
      "source": [
        "# train_dataset = Dataset(train_data, train_labels_clade)\n",
        "# val_dataset = Dataset(val_data, val_labels_clade)\n",
        "# data_size = len(dataset)\n",
        "# train_size = int(data_size * (1-hyperparams['validation_split']))\n",
        "# val_size = data_size - train_size\n",
        "# datasets = {}\n",
        "\n",
        "# datasets[modes[0]], datasets[modes[1]] = random_split(dataset, [train_size, val_size], generator=Generator().manual_seed(hyperparams['split_seed']))\n",
        "\n",
        "\n",
        "datasets = {'train': train_dataset, \"dev\": val_dataset, \"soft\": soft_dataset}\n",
        "\n",
        "data_loader = {\n",
        "    mode: DataLoader(\n",
        "        dataset = datasets[mode],\n",
        "        batch_size = hyperparams['batch_size'],\n",
        "        shuffle = (mode == 'train' or mode == 'soft'),\n",
        "        collate_fn = mix_pad_collate if mode == 'soft' else pad_collate\n",
        "    )\n",
        "    for mode in modes\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Egq36Va16y41",
        "outputId": "073261fc-eb04-4e90-d11c-af93ce7a1471"
      },
      "source": [
        "for (data, data_len, target) in data_loader['dev']:\n",
        "    print(target)\n",
        "\n",
        "    # print(\"data_a: \",data_a)\n",
        "    # print(\"data len_a: \",data_len_a)\n",
        "    # print(\"data_b: \",data_b)\n",
        "    # print(\"data len_b: \",data_len_b)\n",
        "    # print(\"target: \",target)\n",
        "    # print(idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1])\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1])\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1])\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1])\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3])\n",
            "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3])\n",
            "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3])\n",
            "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3])\n",
            "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4])\n",
            "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4])\n",
            "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4])\n",
            "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4])\n",
            "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 4, 4, 4])\n",
            "tensor([4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5])\n",
            "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5])\n",
            "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5])\n",
            "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5])\n",
            "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 6, 6])\n",
            "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 6, 6])\n",
            "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 6, 6])\n",
            "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 6, 6])\n",
            "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7])\n",
            "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7])\n",
            "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7])\n",
            "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7])\n",
            "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8])\n",
            "tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8])\n",
            "tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8])\n",
            "tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8])\n",
            "tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTZo_pZ-ev-a"
      },
      "source": [
        "# Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyykXTp58B-b"
      },
      "source": [
        "def init_weights(module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        nn.init.kaiming_normal_(module.weight.data)\n",
        "        nn.init.normal_(module.bias.data)\n",
        "\n",
        "    elif isinstance(module, nn.Conv2d) or isinstance(module, nn.Conv1d):\n",
        "        nn.init.kaiming_normal_(module.weight, mode='fan_out')\n",
        "\n",
        "    elif isinstance(module, nn.RNNBase) or isinstance(module, nn.LSTMCell) or isinstance(module, nn.GRUCell) or isinstance(module,nn.LSTM):\n",
        "        for name, param in module.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.orthogonal_(param.data)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.normal_(param.data)\n",
        "\n",
        "    elif isinstance(module, nn.BatchNorm1d) or isinstance(module, nn.BatchNorm2d):\n",
        "        module.weight.data.normal_(1.0, 0.02)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ1SbQjuOJlO"
      },
      "source": [
        "\"\"\"\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, hidden_size, nlayers, nclasses=hyperparams['class_num']):\n",
        "        super().__init__()\n",
        "        self.nlayers = nlayers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.nclasses = nclasses\n",
        "        \n",
        "        self.conv1 = torch.nn.Sequential(\n",
        "            nn.Conv1d(1, self.hidden_size, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(self.hidden_size),\n",
        "            nn.ReLU())\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size, num_layers=nlayers,\n",
        "                            bias=True, batch_first=True, dropout=0.0, bidirectional=True)\n",
        "        self.out_layer = nn.Sequential(nn.Dropout(0.2),\n",
        "                            nn.Linear(hidden_size*2, hidden_size),\n",
        "                            nn.Dropout(0.2),\n",
        "                            nn.Linear(hidden_size,self.nclasses))\n",
        "    def forward(self, x, lengths):\n",
        "        x_packed = pack_padded_sequence(x, lengths, enforce_sorted=False)\n",
        "        #print(x_packed)\n",
        "        lstm_packed, hidden = self.lstm(x_packed)\n",
        "        #print(out_packed.shape, hidden.shape)\n",
        "        out, out_lengths = pad_packed_sequence(lstm_packed, batch_first=True)\n",
        "        #print(out.shape, out_lengths.shape)\n",
        "        out = self.out_layer(out).log_softmax(2)\n",
        "        #print(\"got out prob\")\n",
        "        return out, out_lengths\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# hard code with number\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cnn = torch.nn.Sequential(\n",
        "             nn.Conv1d(1, 256, 3, padding=1, bias=False),\n",
        "             nn.BatchNorm1d(256),\n",
        "             nn.SELU(inplace=False),\n",
        "             nn.Conv1d(256,256,3,padding=1,bias=False),\n",
        "             nn.BatchNorm1d(256),\n",
        "             nn.SELU(inplace=False))\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size=256, hidden_size=256, num_layers=3,\n",
        "                            bias=True, dropout=0.0, bidirectional=True)\n",
        "        self.out_layer = nn.Sequential(nn.Dropout(0.2),\n",
        "                            nn.Linear(256*2, 256),\n",
        "                            nn.Dropout(0.2),\n",
        "                            nn.Linear(256,256),\n",
        "                            nn.Dropout(0.2),\n",
        "                            nn.Linear(256, hyperparams['class_num']))\n",
        "        \n",
        "\n",
        "    def forward(self, x, lengths, x_b=None, lengths_b=None, ids=None):\n",
        "        #print(1, x.shape) # x dim = (T,B,F)  (1272,1,1)\n",
        "        x = x.permute(1,2,0) # x dim = (B,F,T) \n",
        "        x = self.cnn(x) # x dim = (B, F', T')\n",
        "        # x = x.permute(2,0,1) # x dim = (T', B, F')\n",
        "\n",
        "        if x_b != None:\n",
        "            x_b = x_b.permute(1,2,0) # x dim = (B,F,T) \n",
        "            x_b = self.cnn(x_b) # x dim = (B, F', T')\n",
        "\n",
        "            for i, idx in enumerate(ids):\n",
        "                _a = alpha[idx]\n",
        "                x[i] = x[i] * _a + x_b[i] * (1-_a)\n",
        "            # x_b = x_b.permute(2,0,1) # x dim = (T', B, F')\n",
        "        \n",
        "        x = x.permute(2,0,1) # x dim = (T', B, F')\n",
        "        # print(x.shape)\n",
        "\n",
        "\n",
        "        x_packed = pack_padded_sequence(x, lengths.cpu(), enforce_sorted=False)\n",
        "        #print(2, x_packed)\n",
        "        lstm_packed, hidden = self.lstm(x_packed)\n",
        "        #print(lstm_packed.shape, hidden.shape)\n",
        "        out, out_lengths = pad_packed_sequence(lstm_packed, batch_first=True)\n",
        "        out = out[:,-1]\n",
        "        #print(3, out.shape, out_lengths.shape)\n",
        "        out = self.out_layer(out)\n",
        "        out = F.softmax(out,dim=1)\n",
        "        #print(4, \"got out prob\")\n",
        "        return out, out_lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITbPx4oUyzDu",
        "outputId": "566186cc-fe45-4754-aa0f-50b7f3726dc6"
      },
      "source": [
        "#model = Net(hidden_size = hyperparams['rnn_hidden_size'], nlayers=hyperparams['rnn_layer_num'])\n",
        "model= Net()\n",
        "model.apply(init_weights)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4, weight_decay=hyperparams['weight_decay'])\n",
        "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "#optimizer_to(optimizer, device)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=2, verbose=True)\n",
        "#scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if cuda else 'cpu')\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (cnn): Sequential(\n",
              "    (0): Conv1d(1, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): SELU()\n",
              "    (3): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
              "    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): SELU()\n",
              "  )\n",
              "  (lstm): LSTM(256, 256, num_layers=3, bidirectional=True)\n",
              "  (out_layer): Sequential(\n",
              "    (0): Dropout(p=0.2, inplace=False)\n",
              "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (2): Dropout(p=0.2, inplace=False)\n",
              "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (4): Dropout(p=0.2, inplace=False)\n",
              "    (5): Linear(in_features=256, out_features=9, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK9UrxjvgcB6"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBraILs5qtNF"
      },
      "source": [
        "from tqdm import tqdm\n",
        "def train_epoch(model,train_loader,criterion,optimizer):\n",
        "  print(\"Training...\")\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  num_correct = 0.0\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  for batch_idx, (data, data_len, target) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    data,data_len = data.to(device),data_len.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    outputs,outputs_len = model(data, data_len)\n",
        "    loss = criterion(outputs,target)\n",
        "    running_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    num_correct += (torch.argmax(outputs, axis=1) == target).sum().item()\n",
        "\n",
        "  end_time = time.time()\n",
        "  running_loss /= len(train_loader)  \n",
        "  acc = (num_correct / len(datasets['train']))*100.0\n",
        "\n",
        "  print(\"Training Loss: \", running_loss, \" Time: \", end_time-start_time, \"s\")\n",
        "  print(\"Training accuracy: \", acc, '%')\n",
        "  return running_loss,acc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaN3v8iB4B5I"
      },
      "source": [
        "def softXEnt(input, target):\n",
        "    logprobs = F.log_softmax(input, dim=1)\n",
        "    return -(target * logprobs).sum() / input.shape[0]\n",
        "\n",
        "def soft_train_epoch(model,soft_loader,criterion,optimizer):\n",
        "  print(\"Training...\")\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  num_correct = 0.0\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  for batch_idx, (data_a, data_len_a, data_b, data_len_b, target, ids) in enumerate(soft_loader):\n",
        "    optimizer.zero_grad()\n",
        "    data_a, data_len_a, data_b, data_len_b, target = data_a.to(device), data_len_a.to(device), data_b.to(device), data_len_b.to(device), target.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    outputs,outputs_len = model(data_a, data_len_a, data_b, data_len_b, ids)\n",
        "    loss = softXEnt(outputs,target)\n",
        "    running_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # num_correct += (torch.argmax(outputs, axis=1) == target).sum().item()\n",
        "\n",
        "  end_time = time.time()\n",
        "  running_loss /= len(soft_loader)  \n",
        "  # acc = (num_correct / len(datasets['train']))*100.0\n",
        "\n",
        "  print(\"Training Loss: \", running_loss, \" Time: \", end_time-start_time, \"s\")\n",
        "  # print(\"Training accuracy: \", acc, '%')\n",
        "  return running_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9Nd7kOQsuMG"
      },
      "source": [
        "def val_model(model, dev_loader, criterion):\n",
        "  print(\"Validating...\")\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    num_correct = 0.0\n",
        "    \n",
        "    for batch_idx, (data,data_len,target) in enumerate(dev_loader):\n",
        "      # print_flag=True\n",
        "      data,data_len = data.to(device),data_len.to(device)\n",
        "      # target = target.reshape(-1) \n",
        "      target = target.to(device)\n",
        "      outputs,outputs_len = model(data,data_len)\n",
        "\n",
        "      # if print_flag:\n",
        "      #   print('argmax', torch.argmax(outputs, axis=1))\n",
        "      #   print('target', target)\n",
        "      #   print_flag=False\n",
        "      # print('output', outputs)\n",
        "\n",
        "\n",
        "      num_correct += (torch.argmax(outputs, axis=1) == target).sum().item()\n",
        "  \n",
        "      loss = criterion(outputs, target).detach()\n",
        "      running_loss += loss.item()\n",
        "    \n",
        "    running_loss /= len(dev_loader)\n",
        "    acc = (num_correct/len(datasets['dev']))*100.0\n",
        "    print(\"Validation Loss: \", running_loss)\n",
        "    print(\"Validation Accuracy: \", acc, \"%\")\n",
        "    return running_loss, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hypfpPBNQqUz",
        "outputId": "334b20df-8390-4d06-9af1-17d0413d5b01"
      },
      "source": [
        "import time\n",
        "\n",
        "\n",
        "NUM_EPOCHES = 50\n",
        "\n",
        "Train_loss = []\n",
        "Valid_loss = []\n",
        "Train_acc = []\n",
        "Valid_acc = []\n",
        "\n",
        "trailNumber = 3\n",
        "\n",
        "\n",
        "for i in range(NUM_EPOCHES):\n",
        "  print(\"Epoch number \", i)\n",
        "  train_loss, train_acc = train_epoch(model, data_loader[\"train\"], criterion, optimizer)\n",
        "  valid_loss, valid_acc = val_model(model, data_loader[\"dev\"], criterion)\n",
        "  scheduler.step(valid_loss)\n",
        "  print(\"learning rate: \", optimizer.param_groups[0]['lr'])\n",
        "\n",
        "  Train_loss.append(train_loss)\n",
        "  Valid_loss.append(valid_loss)\n",
        "  Train_acc.append(train_acc)\n",
        "  Valid_acc.append(valid_acc)\n",
        "\n",
        "  print(\"=\"*20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch number  0\n",
            "Training...\n",
            "Training Loss:  2.1463825380100925  Time:  250.5187680721283 s\n",
            "Training accuracy:  19.945254634954228 %\n",
            "Validating...\n",
            "Validation Loss:  2.1916665905400325\n",
            "Validation Accuracy:  14.312267657992564 %\n",
            "learning rate:  0.0002\n",
            "====================\n",
            "Epoch number  1\n",
            "Training...\n",
            "Training Loss:  2.1310434958514044  Time:  252.8105068206787 s\n",
            "Training accuracy:  20.78253668859548 %\n",
            "Validating...\n",
            "Validation Loss:  2.1851968137841475\n",
            "Validation Accuracy:  13.589425857083851 %\n",
            "learning rate:  0.0002\n",
            "====================\n",
            "Epoch number  2\n",
            "Training...\n",
            "Training Loss:  2.1285995588583106  Time:  251.19672632217407 s\n",
            "Training accuracy:  20.398399043106224 %\n",
            "Validating...\n",
            "Validation Loss:  2.181279166748649\n",
            "Validation Accuracy:  14.39487814952499 %\n",
            "learning rate:  0.0002\n",
            "====================\n",
            "Epoch number  3\n",
            "Training...\n",
            "Training Loss:  2.125732727611766  Time:  248.88385152816772 s\n",
            "Training accuracy:  20.8308414224594 %\n",
            "Validating...\n",
            "Validation Loss:  2.182321630026165\n",
            "Validation Accuracy:  14.39487814952499 %\n",
            "learning rate:  0.0002\n",
            "====================\n",
            "Epoch number  4\n",
            "Training...\n",
            "Training Loss:  2.124630970814649  Time:  253.07595825195312 s\n",
            "Training accuracy:  20.665225192068824 %\n",
            "Validating...\n",
            "Validation Loss:  2.181905536275161\n",
            "Validation Accuracy:  14.39487814952499 %\n",
            "learning rate:  0.0002\n",
            "====================\n",
            "Epoch number  5\n",
            "Training...\n",
            "Training Loss:  2.1233129550428953  Time:  252.44278192520142 s\n",
            "Training accuracy:  20.87914615632332 %\n",
            "Validating...\n",
            "Validation Loss:  2.1811552235954688\n",
            "Validation Accuracy:  14.312267657992564 %\n",
            "Epoch     6: reducing learning rate of group 0 to 1.0000e-04.\n",
            "learning rate:  0.0001\n",
            "====================\n",
            "Epoch number  6\n",
            "Training...\n",
            "Training Loss:  2.121765252421884  Time:  252.13753271102905 s\n",
            "Training accuracy:  20.872245480057046 %\n",
            "Validating...\n",
            "Validation Loss:  2.1809179249562716\n",
            "Validation Accuracy:  14.353572903758776 %\n",
            "learning rate:  0.0001\n",
            "====================\n",
            "Epoch number  7\n",
            "Training...\n",
            "Training Loss:  2.121217337075402  Time:  251.54742884635925 s\n",
            "Training accuracy:  21.104568247688274 %\n",
            "Validating...\n",
            "Validation Loss:  2.1808285995533594\n",
            "Validation Accuracy:  14.39487814952499 %\n",
            "learning rate:  0.0001\n",
            "====================\n",
            "Epoch number  8\n",
            "Training...\n",
            "Training Loss:  2.1215431746314555  Time:  252.90009713172913 s\n",
            "Training accuracy:  21.0700648663569 %\n",
            "Validating...\n",
            "Validation Loss:  2.181593286363702\n",
            "Validation Accuracy:  14.312267657992564 %\n",
            "learning rate:  0.0001\n",
            "====================\n",
            "Epoch number  9\n",
            "Training...\n",
            "Training Loss:  2.1198677147136014  Time:  250.79144549369812 s\n",
            "Training accuracy:  20.991857202005797 %\n",
            "Validating...\n",
            "Validation Loss:  2.179418833632218\n",
            "Validation Accuracy:  14.560099132589837 %\n",
            "learning rate:  0.0001\n",
            "====================\n",
            "Epoch number  10\n",
            "Training...\n",
            "Training Loss:  2.120119265949025  Time:  250.06881880760193 s\n",
            "Training accuracy:  21.171274784928922 %\n",
            "Validating...\n",
            "Validation Loss:  2.179817281271282\n",
            "Validation Accuracy:  14.539446509706732 %\n",
            "learning rate:  0.0001\n",
            "====================\n",
            "Epoch number  11\n",
            "Training...\n",
            "Training Loss:  2.1191791871014765  Time:  253.24086999893188 s\n",
            "Training accuracy:  21.13217095275337 %\n",
            "Validating...\n",
            "Validation Loss:  2.1787679258145785\n",
            "Validation Accuracy:  14.539446509706732 %\n",
            "learning rate:  0.0001\n",
            "====================\n",
            "Epoch number  12\n",
            "Training...\n",
            "Training Loss:  2.1192181320751415  Time:  251.86754155158997 s\n",
            "Training accuracy:  21.021760132492982 %\n",
            "Validating...\n",
            "Validation Loss:  2.179802408343867\n",
            "Validation Accuracy:  13.52746798843453 %\n",
            "learning rate:  0.0001\n",
            "====================\n",
            "Epoch number  13\n",
            "Training...\n",
            "Training Loss:  2.1183544733945063  Time:  252.34570217132568 s\n",
            "Training accuracy:  21.23568109674748 %\n",
            "Validating...\n",
            "Validation Loss:  2.1788945386284277\n",
            "Validation Accuracy:  14.539446509706732 %\n",
            "learning rate:  0.0001\n",
            "====================\n",
            "Epoch number  14\n",
            "Training...\n",
            "Training Loss:  2.1189802660661585  Time:  247.32598304748535 s\n",
            "Training accuracy:  21.233380871325387 %\n",
            "Validating...\n",
            "Validation Loss:  2.1793470508173893\n",
            "Validation Accuracy:  13.816604708798016 %\n",
            "Epoch    15: reducing learning rate of group 0 to 5.0000e-05.\n",
            "learning rate:  5e-05\n",
            "====================\n",
            "Epoch number  15\n",
            "Training...\n",
            "Training Loss:  2.1180445951573987  Time:  238.27802681922913 s\n",
            "Training accuracy:  21.23568109674748 %\n",
            "Validating...\n",
            "Validation Loss:  2.1776514555278577\n",
            "Validation Accuracy:  15.530772408095828 %\n",
            "learning rate:  5e-05\n",
            "====================\n",
            "Epoch number  16\n",
            "Training...\n",
            "Training Loss:  2.1173533530796274  Time:  237.38487935066223 s\n",
            "Training accuracy:  21.691125730321573 %\n",
            "Validating...\n",
            "Validation Loss:  2.175609745477375\n",
            "Validation Accuracy:  15.881866997108633 %\n",
            "learning rate:  5e-05\n",
            "====================\n",
            "Epoch number  17\n",
            "Training...\n",
            "Training Loss:  2.116012503119076  Time:  235.5585880279541 s\n",
            "Training accuracy:  22.14427013847357 %\n",
            "Validating...\n",
            "Validation Loss:  2.179778127293838\n",
            "Validation Accuracy:  15.014456836018175 %\n",
            "learning rate:  5e-05\n",
            "====================\n",
            "Epoch number  18\n",
            "Training...\n",
            "Training Loss:  2.116856338697321  Time:  239.21473598480225 s\n",
            "Training accuracy:  21.813037677692414 %\n",
            "Validating...\n",
            "Validation Loss:  2.1729354356464587\n",
            "Validation Accuracy:  16.460140437835605 %\n",
            "learning rate:  5e-05\n",
            "====================\n",
            "Epoch number  19\n",
            "Training...\n",
            "Training Loss:  2.115322476274827  Time:  238.56169748306274 s\n",
            "Training accuracy:  22.118967658830567 %\n",
            "Validating...\n",
            "Validation Loss:  2.1717141678458765\n",
            "Validation Accuracy:  16.41883519206939 %\n",
            "learning rate:  5e-05\n",
            "====================\n",
            "Epoch number  20\n",
            "Training...\n",
            "Training Loss:  2.114525110581342  Time:  239.0851948261261 s\n",
            "Training accuracy:  22.31678704513042 %\n",
            "Validating...\n",
            "Validation Loss:  2.1731252764400684\n",
            "Validation Accuracy:  15.90251961999174 %\n",
            "learning rate:  5e-05\n",
            "====================\n",
            "Epoch number  21\n",
            "Training...\n",
            "Training Loss:  2.1135583730304943  Time:  237.25263118743896 s\n",
            "Training accuracy:  22.544509361917466 %\n",
            "Validating...\n",
            "Validation Loss:  2.1711789087245337\n",
            "Validation Accuracy:  16.37752994630318 %\n",
            "learning rate:  5e-05\n",
            "====================\n",
            "Epoch number  22\n",
            "Training...\n",
            "Training Loss:  2.113101826695835  Time:  238.9233114719391 s\n",
            "Training accuracy:  22.45940102130009 %\n",
            "Validating...\n",
            "Validation Loss:  2.1708456968006336\n",
            "Validation Accuracy:  16.625361420900454 %\n",
            "learning rate:  5e-05\n",
            "====================\n",
            "Epoch number  23\n",
            "Training...\n",
            "Training Loss:  2.113479460688198  Time:  237.2049720287323 s\n",
            "Training accuracy:  22.59511432120348 %\n",
            "Validating...\n",
            "Validation Loss:  2.1711946568991007\n",
            "Validation Accuracy:  16.37752994630318 %\n",
            "learning rate:  5e-05\n",
            "====================\n",
            "Epoch number  24\n",
            "Training...\n",
            "Training Loss:  2.1130844957688275  Time:  235.2215392589569 s\n",
            "Training accuracy:  22.544509361917466 %\n",
            "Validating...\n",
            "Validation Loss:  2.1705014078240645\n",
            "Validation Accuracy:  16.52209830648492 %\n",
            "learning rate:  5e-05\n",
            "====================\n",
            "Epoch number  25\n",
            "Training...\n",
            "Training Loss:  2.113155617433436  Time:  239.27326917648315 s\n",
            "Training accuracy:  22.521507107696554 %\n",
            "Validating...\n",
            "Validation Loss:  2.170494318008423\n",
            "Validation Accuracy:  16.37752994630318 %\n",
            "learning rate:  5e-05\n",
            "====================\n",
            "Epoch number  26\n",
            "Training...\n",
            "Training Loss:  2.1147869664080003  Time:  238.04333519935608 s\n",
            "Training accuracy:  22.09366517918756 %\n",
            "Validating...\n",
            "Validation Loss:  2.1725912501937463\n",
            "Validation Accuracy:  15.964477488641057 %\n",
            "learning rate:  5e-05\n",
            "====================\n",
            "Epoch number  27\n",
            "Training...\n",
            "Training Loss:  2.1135657633052154  Time:  238.04927110671997 s\n",
            "Training accuracy:  22.33058839766297 %\n",
            "Validating...\n",
            "Validation Loss:  2.170783478962748\n",
            "Validation Accuracy:  16.294919454770756 %\n",
            "Epoch    28: reducing learning rate of group 0 to 2.5000e-05.\n",
            "learning rate:  2.5e-05\n",
            "====================\n",
            "Epoch number  28\n",
            "Training...\n",
            "Training Loss:  2.113768287967233  Time:  237.45791625976562 s\n",
            "Training accuracy:  22.305285918019965 %\n",
            "Validating...\n",
            "Validation Loss:  2.1701220273971558\n",
            "Validation Accuracy:  16.33622470053697 %\n",
            "learning rate:  2.5e-05\n",
            "====================\n",
            "Epoch number  29\n",
            "Training...\n",
            "Training Loss:  2.1140151753145107  Time:  238.7680606842041 s\n",
            "Training accuracy:  22.420297189124533 %\n",
            "Validating...\n",
            "Validation Loss:  2.170696911058928\n",
            "Validation Accuracy:  16.088393225939697 %\n",
            "learning rate:  2.5e-05\n",
            "====================\n",
            "Epoch number  30\n",
            "Training...\n",
            "Training Loss:  2.11372608647627  Time:  237.12274765968323 s\n",
            "Training accuracy:  22.339789299351338 %\n",
            "Validating...\n",
            "Validation Loss:  2.1703460247893083\n",
            "Validation Accuracy:  16.33622470053697 %\n",
            "learning rate:  2.5e-05\n",
            "====================\n",
            "Epoch number  31\n",
            "Training...\n",
            "Training Loss:  2.113585547839894  Time:  235.08401012420654 s\n",
            "Training accuracy:  22.296085016331602 %\n",
            "Validating...\n",
            "Validation Loss:  2.1701389300195792\n",
            "Validation Accuracy:  16.109045848822802 %\n",
            "Epoch    32: reducing learning rate of group 0 to 1.2500e-05.\n",
            "learning rate:  1.25e-05\n",
            "====================\n",
            "Epoch number  32\n",
            "Training...\n",
            "Training Loss:  2.112801316205193  Time:  239.02149152755737 s\n",
            "Training accuracy:  22.475502599254725 %\n",
            "Validating...\n",
            "Validation Loss:  2.169904417113254\n",
            "Validation Accuracy:  16.33622470053697 %\n",
            "learning rate:  1.25e-05\n",
            "====================\n",
            "Epoch number  33\n",
            "Training...\n",
            "Training Loss:  2.1118866871385014  Time:  237.76861119270325 s\n",
            "Training accuracy:  22.652619956755764 %\n",
            "Validating...\n",
            "Validation Loss:  2.169761293812802\n",
            "Validation Accuracy:  16.480793060718714 %\n",
            "learning rate:  1.25e-05\n",
            "====================\n",
            "Epoch number  34\n",
            "Training...\n",
            "Training Loss:  2.1123832555378184  Time:  238.34839391708374 s\n",
            "Training accuracy:  22.526107558540737 %\n",
            "Validating...\n",
            "Validation Loss:  2.169690596429925\n",
            "Validation Accuracy:  16.60470879801735 %\n",
            "learning rate:  1.25e-05\n",
            "====================\n",
            "Epoch number  35\n",
            "Training...\n",
            "Training Loss:  2.1125488063868354  Time:  236.92996168136597 s\n",
            "Training accuracy:  22.526107558540737 %\n",
            "Validating...\n",
            "Validation Loss:  2.169446073080364\n",
            "Validation Accuracy:  16.60470879801735 %\n",
            "learning rate:  1.25e-05\n",
            "====================\n",
            "Epoch number  36\n",
            "Training...\n",
            "Training Loss:  2.113107505966635  Time:  238.27729272842407 s\n",
            "Training accuracy:  22.69862446519759 %\n",
            "Validating...\n",
            "Validation Loss:  2.168518022487038\n",
            "Validation Accuracy:  16.790582403965303 %\n",
            "learning rate:  1.25e-05\n",
            "====================\n",
            "Epoch number  37\n",
            "Training...\n",
            "Training Loss:  2.1105948756722843  Time:  236.76856398582458 s\n",
            "Training accuracy:  22.90564475318581 %\n",
            "Validating...\n",
            "Validation Loss:  2.16922810830568\n",
            "Validation Accuracy:  16.64601404378356 %\n",
            "learning rate:  1.25e-05\n",
            "====================\n",
            "Epoch number  38\n",
            "Training...\n",
            "Training Loss:  2.112939178943634  Time:  234.99902176856995 s\n",
            "Training accuracy:  22.57901274324884 %\n",
            "Validating...\n",
            "Validation Loss:  2.168887536776693\n",
            "Validation Accuracy:  16.70797191243288 %\n",
            "learning rate:  1.25e-05\n",
            "====================\n",
            "Epoch number  39\n",
            "Training...\n",
            "Training Loss:  2.111976480484009  Time:  238.63349199295044 s\n",
            "Training accuracy:  22.64801950591158 %\n",
            "Validating...\n",
            "Validation Loss:  2.168928127539785\n",
            "Validation Accuracy:  16.666666666666664 %\n",
            "Epoch    40: reducing learning rate of group 0 to 6.2500e-06.\n",
            "learning rate:  6.25e-06\n",
            "====================\n",
            "Epoch number  40\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-23-2e86a4fcd328>\", line 16, in <module>\n",
            "    train_loss, train_acc = train_epoch(model, data_loader[\"train\"], criterion, optimizer)\n",
            "  File \"<ipython-input-19-2518ee605f00>\", line 18, in train_epoch\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/tensor.py\", line 245, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 147, in backward\n",
            "    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 725, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 709, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 383, in abspath\n",
            "    cwd = os.getcwd()\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvxohEtRdVTy"
      },
      "source": [
        "torch.save(model.state_dict(),\"/content/drive/MyDrive/Colab Notebooks/project/model3_bal.t7\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S98TaosdCoD"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(Train_loss)\n",
        "plt.plot(Valid_loss)\n",
        "plt.legend([\"Training loss\", \"Validation loss\"])\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Model Loss\")\n",
        "plt.savefig(\"/content/drive/MyDrive/Colab Notebooks/project/model3_balanced_train_valid_loss.png\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(Train_acc)\n",
        "plt.plot(Valid_acc)\n",
        "plt.legend([\"Training accuracy\", \"Validation accuracy\"])\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model Accuracy\")\n",
        "plt.savefig(\"/content/drive/MyDrive/Colab Notebooks/project/model3_balanced_train_valid_acc.png\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87J-lJ6fPcWX"
      },
      "source": [
        "print(Train_loss)\n",
        "print(Valid_loss)\n",
        "print(Train_acc)\n",
        "print(Valid_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OWHqt8LeeIl",
        "outputId": "3f796d43-cb3a-4850-8e92-84be795d6476"
      },
      "source": [
        "# from previously\n",
        "print(Train_loss)\n",
        "print(Valid_loss)\n",
        "print(Train_acc)\n",
        "print(Valid_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.1392787190044626, 2.1313355249517105, 2.127023722143734, 2.1257093071937563, 2.124636416575488, 2.1226815938949586, 2.123012591109556, 2.1222676831133227, 2.1228218169773325, 2.1214712493559893, 2.118426306808696, 2.1199606404585, 2.1196013324400957, 2.1183928524746616, 2.1188916024039774, 2.1189867727896745, 2.1186048598850475, 2.1175557634409734, 2.1188400983810425, 2.1209354933570412, 2.121075086733874, 2.1210198633811053, 2.1204829798025244, 2.120311984244515, 2.1169411357711345, 2.1144739508628847, 2.114802622795105, 2.113264267584857, 2.1128294762443094, 2.1117518971948064]\n",
            "[2.183298857588517, 2.182999748932688, 2.184594734718925, 2.183501334566819, 2.182027882651279, 2.1848321870753638, 2.181132871853678, 2.181896184620104, 2.18109835449018, 2.1796724012023523, 2.181877694631878, 2.1796338871905676, 2.1807382200893604, 2.179370895812386, 2.1785445338801335, 2.179228236800746, 2.1770836742300737, 2.175572875298952, 2.180401883627239, 2.1813284660640515, 2.1808434975774666, 2.1804879245005155, 2.180739402770996, 2.180333517099682, 2.174942788324858, 2.17299695391404, 2.172330906516627, 2.1716332968912626, 2.172280292761953, 2.169564215760482]\n",
            "[20.536412568431707, 20.584717302295623, 20.973455398629063, 20.888347058011686, 20.667525417490914, 20.8308414224594, 20.72273082762111, 21.127570501909187, 20.84694300041404, 20.943552468141878, 21.226480195059118, 20.81243961908267, 21.086166444311544, 21.074665317201084, 21.221879744214935, 20.84694300041404, 21.366793945806688, 21.730229562497126, 21.25178267470212, 20.750333532686202, 20.823940746193127, 20.639922712425818, 20.821640520771034, 21.021760132492982, 21.500207020287988, 22.204075999447944, 22.385793807793164, 22.56751161613838, 22.652619956755764, 22.827437088834706]\n",
            "[14.353572903758776, 14.312267657992564, 14.39487814952499, 13.589425857083851, 14.39487814952499, 13.589425857083851, 14.332920280875673, 14.312267657992564, 14.312267657992564, 14.539446509706732, 14.580751755472946, 14.518793886823628, 14.560099132589837, 14.601404378356051, 15.241635687732341, 14.580751755472946, 16.088393225939697, 16.06774060305659, 14.312267657992564, 14.39487814952499, 14.353572903758776, 14.312267657992564, 14.353572903758776, 14.312267657992564, 16.232961586121437, 16.171003717472118, 16.150351094589013, 16.31557207765386, 16.00578273440727, 16.687319289549773]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MSBfJJ5e-ij"
      },
      "source": [
        "# Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp87AGSkfBhM"
      },
      "source": [
        "def init_weights(module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        nn.init.kaiming_normal_(module.weight.data)\n",
        "        nn.init.normal_(module.bias.data)\n",
        "\n",
        "    elif isinstance(module, nn.Conv2d) or isinstance(module, nn.Conv1d):\n",
        "        nn.init.kaiming_normal_(module.weight, mode='fan_out')\n",
        "\n",
        "    elif isinstance(module, nn.RNNBase) or isinstance(module, nn.LSTMCell) or isinstance(module, nn.GRUCell):\n",
        "        for name, param in module.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.orthogonal_(param.data)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.normal_(param.data)\n",
        "\n",
        "    elif isinstance(module, nn.BatchNorm1d) or isinstance(module, nn.BatchNorm2d):\n",
        "        module.weight.data.normal_(1.0, 0.02)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb9ohZ03shft"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}