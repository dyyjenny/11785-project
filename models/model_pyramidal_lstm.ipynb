{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchsummaryX import summary\n",
    "\n",
    "# Import utility libraries for evaluating model results\n",
    "import csv\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda = True with num_workers = 8\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "num_workers = 8 if cuda else 0\n",
    "print(\"Cuda = %s with num_workers = %d\" % (cuda, num_workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into numpy\n",
    "\n",
    "def load_data(in_filename):\n",
    "    data = np.load(in_filename, allow_pickle=True)\n",
    "    for i in range(data.shape[0]):\n",
    "        data[i] = data[i].astype(np.float32)\n",
    "    return data\n",
    "\n",
    "def load_labels(in_filename):\n",
    "    data = np.load(in_filename, allow_pickle=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into numpy\n",
    "\n",
    "x_train = load_data(\"data/train_data_balanced.npy\")\n",
    "labels_train = load_labels(\"data/train_label_balanced.npy\")\n",
    "\n",
    "x_val = load_data(\"data/validation_data_balanced.npy\")\n",
    "labels_val = load_labels(\"data/validation_label_balanced.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (43434,) object\n",
      "Training labels shape:  (43434,) int64\n",
      "Validation data shape:  (4842,) object\n",
      "Validation labels shape:  (4842,) int64\n"
     ]
    }
   ],
   "source": [
    "# Print data shape\n",
    "\n",
    "print(\"Training data shape: \", x_train.shape, x_train.dtype)\n",
    "print(\"Training labels shape: \", labels_train.shape, labels_train.dtype)\n",
    "\n",
    "print(\"Validation data shape: \", x_val.shape, x_val.dtype)\n",
    "print(\"Validation labels shape: \", labels_val.shape, labels_val.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset definition\n",
    "\n",
    "class LSTMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = torch.Tensor(self.x[index]).long()\n",
    "        y = self.y[index]\n",
    "        return (x,y)\n",
    "    \n",
    "\n",
    "def pad_collate(batch):\n",
    "    # batch: A list of length batch_size, where each entry is an (x,y) tuple\n",
    "    # xs: A list of length batch_size, where each entry is a tensor of x-values\n",
    "    # ys: A list of length batch_size, where each entry is a tensor of y-values\n",
    "    (xs,y) = zip(*batch)\n",
    "\n",
    "    x_lengths = torch.Tensor([len(x) for x in xs]).long()\n",
    "    xs = torch.nn.utils.rnn.pad_sequence(xs, batch_first=False, padding_value=0)\n",
    "    y = torch.Tensor(y).long()\n",
    "    \n",
    "    return (xs, x_lengths, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "\n",
    "dataloader_args = {\n",
    "    \"shuffle\": True,\n",
    "    \"batch_size\": 32,\n",
    "    \"drop_last\": True,\n",
    "    \"pin_memory\": True,\n",
    "    \"num_workers\": 0,\n",
    "    \"collate_fn\": pad_collate,\n",
    "}\n",
    "\n",
    "# Training dataloader\n",
    "train_data = LSTMDataset(x_train, labels_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, **dataloader_args)\n",
    "\n",
    "# Validation dataloader\n",
    "val_data = LSTMDataset(x_val, labels_val)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, **dataloader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "def train_model(train_loader, model, optimizer, criterion):\n",
    "    training_loss = 0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    model.train()\n",
    "    for (inputs, input_lengths, targets) in train_loader:\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        (out, out_lengths) = model(inputs, input_lengths)\n",
    "\n",
    "        out = out[out_lengths[0]-1,:,:]\n",
    "        loss = criterion(out, targets)\n",
    "        training_loss += loss.item()\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        out = out.detach()\n",
    "        out = torch.argmax(out, 1)\n",
    "        out = torch.reshape(out, (out.numel(), 1))\n",
    "        predictions.append(out)\n",
    "        \n",
    "        actual = targets.detach()\n",
    "        actual = torch.reshape(actual, (actual.numel(), 1))\n",
    "        actuals.append(actual)\n",
    "\n",
    "    predictions = torch.cat(predictions, 0).cpu().numpy()\n",
    "    actuals = torch.cat(actuals, 0).cpu().numpy()\n",
    "    training_loss /= len(train_loader)\n",
    "    \n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return (acc, training_loss, predictions, actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "def evaluate_model(val_loader, model, criterion):\n",
    "    validation_loss = 0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (inputs, input_lengths, targets) in val_loader:\n",
    "            if cuda:\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "                \n",
    "            (out, out_lengths) = model(inputs, input_lengths)\n",
    "            \n",
    "            out = out[out_lengths[0]-1,:,:]\n",
    "            loss = criterion(out, targets)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            out = out.detach()\n",
    "            out = torch.argmax(out, 1)\n",
    "            out = torch.reshape(out, (len(out), 1))\n",
    "            predictions.append(out)\n",
    "            \n",
    "            actual = targets.detach()\n",
    "            actual = torch.reshape(actual, (len(actual), 1))\n",
    "            actuals.append(actual)\n",
    "\n",
    "        predictions = torch.cat(predictions, 0).cpu().numpy()\n",
    "        actuals = torch.cat(actuals, 0).cpu().numpy()\n",
    "        validation_loss /= len(val_loader)\n",
    "        \n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return (acc, validation_loss, predictions, actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "\n",
    "# Pyramidal BiLSTM\n",
    "# Each layer reduces time resolution by a given reduction factor\n",
    "class pBLSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, reduction_factor):\n",
    "        super(pBLSTM, self).__init__()\n",
    "        self.reduction_factor = reduction_factor\n",
    "        self.blstm = torch.nn.LSTM(input_size * reduction_factor, hidden_size, num_layers=1, batch_first=False, bidirectional=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        r = self.reduction_factor\n",
    "        \n",
    "        x = x.permute(1,2,0) # From (seq_len, batch_size, input_size) to (batch_size, input_size, seq_len)\n",
    "        (batch_size, input_size, seq_len) = x.shape\n",
    "        \n",
    "        x = x[:,:,:(r*(seq_len//r))]\n",
    "        x = x.view(batch_size, input_size, seq_len//r, r)\n",
    "        x = x.permute(0,1,3,2) # From (batch_size, input_size, seq_len//r, r) to (batch_size, input_size, r, seq_len//r)\n",
    "        x = x.reshape(batch_size, input_size*r, seq_len//r)\n",
    "        \n",
    "        x = x.permute(2,0,1) # From (batch_size, input_size, seq_len) to (seq_len, batch_size, input_size)\n",
    "        out = self.blstm(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "\n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, in_channels, in_classes, out_classes, embedding_size, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(in_classes, embedding_size)\n",
    "        \n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(embedding_size, hidden_size, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm1d(hidden_size),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(hidden_size, hidden_size, num_layers=1, batch_first=False, bidirectional=True)\n",
    "        self.pblstm1 = pBLSTM(2*hidden_size, hidden_size, 2)\n",
    "        self.pblstm2 = pBLSTM(2*hidden_size, hidden_size, 2)\n",
    "        self.pblstm3 = pBLSTM(2*hidden_size, hidden_size, 2)\n",
    "        \n",
    "        self.linear = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2*hidden_size, hidden_size),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(hidden_size, out_classes),\n",
    "            torch.nn.LogSoftmax(dim=2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, x_lengths):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        x = x.permute(1,2,0) # From (seq_len, batch_size, input_size) to (batch_size, input_size, seq_len)\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(2,0,1) # From (batch_size, input_size, seq_len) to (seq_len, batch_size, input_size)\n",
    "        \n",
    "        (x, (_, _)) = self.lstm(x)\n",
    "        (x, (_, _)) = self.pblstm1(x)\n",
    "        (x, (_, _)) = self.pblstm2(x)\n",
    "        (x, (_, _)) = self.pblstm3(x)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        return (x, x_lengths // 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "in_channels = 1\n",
    "in_classes = 25 # Length of amino mapping\n",
    "out_classes = np.max(labels_train) + 1 # Add 1 to account for blank\n",
    "model = LSTM(in_channels, in_classes, out_classes, 32, 128)\n",
    "\n",
    "# Criterion / loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2500)\n",
    "\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "    \n",
    "# Print model\n",
    "# print(model)\n",
    "# print(criterion)\n",
    "# print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-414464f702e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Training and validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_actuals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_actuals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-10a9f4107239>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, model, optimizer, criterion)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtraining_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 80\n",
    "lr_stop = 0.00005\n",
    "\n",
    "train_accs = []\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "val_losses = []\n",
    "\n",
    "start_timestamp = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "for epoch in range(epochs):\n",
    "    # Start timer\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Training and validation\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    (train_acc, train_loss, train_predictions, train_actuals) = train_model(train_loader, model, optimizer, criterion)\n",
    "    (val_acc, val_loss, val_predictions, val_actuals) = evaluate_model(val_loader, model, criterion)\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    # Print log\n",
    "    time_elapsed = time.perf_counter() - start_time\n",
    "    timestamp = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    print(\"Epoch: %d, Training loss: %.3f, Training accuracy: %.2f%%, Validation loss: %.3f, Validation accuracy: %.2f%%, Learning rate: %f, Time elapsed: %.3f, Timestamp: %s\" %\n",
    "          (epoch, train_loss, 100*train_acc, val_loss, 100*val_acc, lr, time_elapsed, timestamp))\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    # Early stopping\n",
    "    if lr < lr_stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "                        Kernel Shape     Output Shape    Params   Mult-Adds\n",
      "Layer                                                                      \n",
      "0_embedding                 [32, 25]   [1274, 32, 32]     800.0       800.0\n",
      "1_conv.Conv1d_0         [32, 128, 3]  [32, 128, 1274]   12.288k  15.654912M\n",
      "2_conv.BatchNorm1d_1           [128]  [32, 128, 1274]     256.0       128.0\n",
      "3_conv.ReLU_2                      -  [32, 128, 1274]         -           -\n",
      "4_lstm                             -  [1274, 32, 256]  264.192k    262.144k\n",
      "5_pblstm1.LSTM_blstm               -   [637, 32, 256]  657.408k     655.36k\n",
      "6_pblstm2.LSTM_blstm               -   [318, 32, 256]  657.408k     655.36k\n",
      "7_pblstm3.LSTM_blstm               -   [159, 32, 256]  657.408k     655.36k\n",
      "8_linear.Linear_0         [256, 128]   [159, 32, 128]   32.896k     32.768k\n",
      "9_linear.Dropout_1                 -   [159, 32, 128]         -           -\n",
      "10_linear.Linear_2          [128, 9]     [159, 32, 9]    1.161k      1.152k\n",
      "11_linear.LogSoftmax_3             -     [159, 32, 9]         -           -\n",
      "---------------------------------------------------------------------------\n",
      "                          Totals\n",
      "Total params           2.283817M\n",
      "Trainable params       2.283817M\n",
      "Non-trainable params         0.0\n",
      "Mult-Adds             17.917984M\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "# Print model torchsummary\n",
    "for (inputs, input_lengths, targets) in train_loader:\n",
    "    if cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    summary(model, 0*inputs, x_lengths=input_lengths)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure()\n",
    "plt.plot(range(len(train_accs)), train_accs, label=\"Training accuracy\")\n",
    "plt.plot(range(len(val_accs)), val_accs, label=\"Validation accuracy\")\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.xticks(range(0, len(train_accs), 2))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(train_accs)), train_losses, label=\"Training losses\")\n",
    "plt.plot(range(len(val_losses)), val_losses, label=\"Validation losses\")\n",
    "plt.title(\"Model Losses\")\n",
    "plt.xticks(range(0, len(train_accs), 2))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(train_actuals, range(out_classes+1), density=True, alpha=0.5, label=\"Ground truth\")\n",
    "plt.hist(train_predictions, range(out_classes+1), density=True, alpha=0.5, label=\"Model output\")\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.title(\"Label Frequencies (Training)\")\n",
    "plt.xticks(range(out_classes))\n",
    "plt.xlabel(\"Output class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(val_actuals, range(out_classes+1), density=True, alpha=0.5, label=\"Ground truth\")\n",
    "plt.hist(val_predictions, range(out_classes+1), density=True, alpha=0.5, label=\"Model output\")\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.title(\"Label Frequencies (Validation)\")\n",
    "plt.xticks(range(out_classes))\n",
    "plt.xlabel(\"Output class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "\n",
    "train_confusion = np.zeros((out_classes, out_classes))\n",
    "for (prediction, actual) in zip(train_predictions, train_actuals):\n",
    "    train_confusion[actual, prediction] += 1\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(train_confusion, cmap=\"gist_heat\", interpolation=\"nearest\")\n",
    "plt.title(\"Confusion Matrix (Training)\")\n",
    "plt.xticks(range(out_classes))\n",
    "plt.xlabel(\"Model output\")\n",
    "plt.ylabel(\"Ground truth label\")\n",
    "plt.show()\n",
    "\n",
    "val_confusion = np.zeros((out_classes, out_classes))\n",
    "for (prediction, actual) in zip(val_predictions, val_actuals):\n",
    "    val_confusion[actual, prediction] += 1\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(val_confusion, cmap=\"gist_heat\", interpolation=\"nearest\")\n",
    "plt.title(\"Confusion Matrix (Validation)\")\n",
    "plt.xticks(range(out_classes))\n",
    "plt.xlabel(\"Model output\")\n",
    "plt.ylabel(\"Ground truth label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
