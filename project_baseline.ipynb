{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from sklearn.metrics import accuracy_score\n",
    "import Levenshtein\n",
    "\n",
    "# Import utility libraries for evaluating model results\n",
    "import csv\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if cuda is available\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "use_amp = False\n",
    "num_workers = 8 if cuda else 0\n",
    "print(\"Cuda = %s with num_workers = %d\" % (cuda, num_workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into numpy\n",
    "\n",
    "def load_data(in_filename):\n",
    "    data = np.load(in_filename, allow_pickle=True)\n",
    "    for i in range(data.shape[0]):\n",
    "        data[i] = data[i].astype(np.float32)\n",
    "    return data\n",
    "\n",
    "def load_labels(in_filename):\n",
    "    data = np.load(in_filename, allow_pickle=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into numpy\n",
    "\n",
    "x_train = load_data(\"../../project/data/train_data_balanced.npy\")\n",
    "labels_train = load_labels(\"../../project/data/train_label_balanced.npy\")\n",
    "\n",
    "x_val = load_data(\"../../project/data/validation_data_balanced.npy\")\n",
    "labels_val = load_labels(\"../../project/data/validation_label_balanced.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get smaller subset of data to verify code correctness\n",
    "\n",
    "def data_subset(x, labels, subset_size):\n",
    "    assert(x.shape[0] == labels.shape[0])\n",
    "    indices = np.random.randint(x.shape[0], size=subset_size)\n",
    "    return (x[indices], labels[indices])\n",
    "\n",
    "(x_subset, labels_subset) = data_subset(x_val, labels_val, 43432)\n",
    "(x_subset_val, labels_subset_val) = data_subset(x_val, labels_val, 4842)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print data shape\n",
    "\n",
    "print(\"Training data shape: \", x_train.shape, x_train.dtype)\n",
    "print(\"Training labels shape: \", labels_train.shape, labels_train.dtype)\n",
    "\n",
    "print(\"Validation data shape: \", x_val.shape, x_val.dtype)\n",
    "print(\"Validation labels shape: \", labels_val.shape, labels_val.dtype)\n",
    "\n",
    "print(\"Subset data shape: \", x_subset.shape, x_subset.dtype)\n",
    "print(\"Subset labels shape: \", labels_subset.shape, labels_subset.dtype)\n",
    "\n",
    "print(\"Subset validation data shape: \", x_subset_val.shape, x_subset.dtype)\n",
    "print(\"Subset validation labels shape: \", labels_subset_val.shape, labels_subset_val.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset definition\n",
    "\n",
    "class LSTMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = torch.Tensor(self.x[index]).long()\n",
    "        y = self.y[index]\n",
    "        return (x,y)\n",
    "    \n",
    "\n",
    "def pad_collate(batch):\n",
    "    # batch: A list of length batch_size, where each entry is an (x,y) tuple\n",
    "    # xs: A list of length batch_size, where each entry is a tensor of x-values\n",
    "    # ys: A list of length batch_size, where each entry is a tensor of y-values\n",
    "    (xs,y) = zip(*batch)\n",
    "\n",
    "    x_lengths = torch.Tensor([len(x) for x in xs]).long()\n",
    "    xs = torch.nn.utils.rnn.pad_sequence(xs, batch_first=False, padding_value=0)\n",
    "    y = torch.Tensor(y).long()\n",
    "    \n",
    "    return (xs, x_lengths, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "\n",
    "dataloader_args = {\n",
    "    \"shuffle\": True,\n",
    "    \"batch_size\": 64,\n",
    "    \"drop_last\": True,\n",
    "    \"pin_memory\": True,\n",
    "    \"num_workers\": 0,\n",
    "    \"collate_fn\": pad_collate,\n",
    "}\n",
    "\n",
    "testloader_args = {\n",
    "    \"shuffle\": False,\n",
    "    \"batch_size\": 64,\n",
    "    \"drop_last\": False,\n",
    "    \"pin_memory\": True,\n",
    "    \"num_workers\": 0,\n",
    "    \"collate_fn\": pad_collate,\n",
    "}\n",
    "\n",
    "# Training dataloader\n",
    "train_data = LSTMDataset(x_train, labels_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, **dataloader_args)\n",
    "\n",
    "# Validation dataloader\n",
    "val_data = LSTMDataset(x_val, labels_val)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, **dataloader_args)\n",
    "\n",
    "# Subset dataloader\n",
    "subset_data = LSTMDataset(x_subset, labels_subset)\n",
    "subset_loader = torch.utils.data.DataLoader(subset_data, **dataloader_args)\n",
    "\n",
    "# Subset validation dataloader\n",
    "subset_val_data = LSTMDataset(x_subset_val, labels_subset_val)\n",
    "subset_val_loader = torch.utils.data.DataLoader(subset_val_data, **dataloader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "def save_model(model, optimizer, scaler, timestamp_str):\n",
    "    torch.save({\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scaler\": scaler.state_dict()\n",
    "    }, \"models/model_%s.pt\" % timestamp_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "def train_model(train_loader, model, optimizer, criterion, scaler):\n",
    "    training_loss = 0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    model.train()\n",
    "    for (inputs, input_lengths, targets) in train_loader:\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            (out, out_lengths) = model(inputs, input_lengths)\n",
    "\n",
    "        out = out[out_lengths[0]-1,:,:]        \n",
    "#         out = out.permute(1,2,0) # From (seq_len, batch_size, num_classes) to (batch_size, num_classes, seq_len)\n",
    "#         targets = targets.unsqueeze(1).repeat(1, out.shape[2]) # From (batch_size) to (batch_size, seq_len)\n",
    "        loss = criterion(out, targets)\n",
    "        training_loss += loss.item()\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        out = out.detach()\n",
    "        out = torch.argmax(out, 1)\n",
    "        out = torch.reshape(out, (out.numel(), 1))\n",
    "        predictions.append(out)\n",
    "        \n",
    "        actual = targets.detach()\n",
    "        actual = torch.reshape(actual, (actual.numel(), 1))\n",
    "        actuals.append(actual)\n",
    "\n",
    "    predictions = torch.cat(predictions, 0).cpu().numpy()\n",
    "    actuals = torch.cat(actuals, 0).cpu().numpy()\n",
    "    training_loss /= len(train_loader)\n",
    "    \n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return (acc, training_loss, predictions, actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "def evaluate_model(val_loader, model, criterion):\n",
    "    validation_loss = 0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (inputs, input_lengths, targets) in val_loader:\n",
    "            if cuda:\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "                \n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                (out, out_lengths) = model(inputs, input_lengths)\n",
    "            \n",
    "            out = out[out_lengths[0]-1,:,:]\n",
    "            loss = criterion(out, targets)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            out = out.detach()\n",
    "            out = torch.argmax(out, 1)\n",
    "            out = torch.reshape(out, (len(out), 1))\n",
    "            predictions.append(out)\n",
    "            \n",
    "            actual = targets.detach()\n",
    "            actual = torch.reshape(actual, (len(actual), 1))\n",
    "            actuals.append(actual)\n",
    "\n",
    "        predictions = torch.cat(predictions, 0).cpu().numpy()\n",
    "        actuals = torch.cat(actuals, 0).cpu().numpy()\n",
    "        validation_loss /= len(val_loader)\n",
    "        \n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return (acc, validation_loss, predictions, actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "\n",
    "# Pyramidal BiLSTM\n",
    "# Each layer reduces time resolution by a given reduction factor\n",
    "class pBLSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, reduction_factor):\n",
    "        super(pBLSTM, self).__init__()\n",
    "        self.reduction_factor = reduction_factor\n",
    "        self.blstm = torch.nn.LSTM(input_size * reduction_factor, hidden_size, num_layers=1, batch_first=False, bidirectional=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        r = self.reduction_factor\n",
    "        \n",
    "        x = x.permute(1,2,0) # From (seq_len, batch_size, input_size) to (batch_size, input_size, seq_len)\n",
    "        (batch_size, input_size, seq_len) = x.shape\n",
    "        \n",
    "        x = x[:,:,:(r*(seq_len//r))]\n",
    "        x = x.view(batch_size, input_size, seq_len//r, r)\n",
    "        x = x.permute(0,1,3,2) # From (batch_size, input_size, seq_len//r, r) to (batch_size, input_size, r, seq_len//r)\n",
    "        x = x.reshape(batch_size, input_size*r, seq_len//r)\n",
    "        \n",
    "        x = x.permute(2,0,1) # From (batch_size, input_size, seq_len) to (seq_len, batch_size, input_size)\n",
    "        out = self.blstm(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "\n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, in_channels, in_classes, out_classes, embedding_size, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(1, hidden_size, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            torch.nn.BatchNorm1d(hidden_size),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(in_classes, embedding_size)\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(hidden_size, hidden_size, num_layers=3, batch_first=False, bidirectional=True)\n",
    "#         self.pblstm1 = pBLSTM(2*hidden_size, hidden_size, 1)\n",
    "#         self.pblstm2 = pBLSTM(2*hidden_size, hidden_size, 1)\n",
    "#         self.pblstm3 = pBLSTM(2*hidden_size, hidden_size, 1)\n",
    "        \n",
    "        self.linear = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2*hidden_size, hidden_size),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(hidden_size, out_classes),\n",
    "            torch.nn.LogSoftmax(dim=2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, x_lengths):\n",
    "        # x = self.embedding(x)\n",
    "        x = x.unsqueeze(2).float()\n",
    "        \n",
    "        x = x.permute(1,2,0) # From (seq_len, batch_size, input_size) to (batch_size, input_size, seq_len)\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(2,0,1) # From (batch_size, input_size, seq_len) to (seq_len, batch_size, input_size)\n",
    "        \n",
    "        (x, (_, _)) = self.lstm(x)\n",
    "#         (x, (_, _)) = self.pblstm1(x)\n",
    "#         (x, (_, _)) = self.pblstm2(x)\n",
    "#         (x, (_, _)) = self.pblstm3(x)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        return (x, x_lengths // 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "in_channels = 1\n",
    "in_classes = 25 # Length of amino mapping\n",
    "out_classes = np.max(labels_train) + 1 # Add 1 to account for blank\n",
    "model = LSTM(in_channels, in_classes, out_classes, 32, 128)\n",
    "\n",
    "# Criterion / loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2500)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "    \n",
    "# Print model\n",
    "# print(model)\n",
    "# print(criterion)\n",
    "# print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "lr_stop = 0.00005\n",
    "\n",
    "# with torch.autograd.profiler.profile(record_shapes=True, use_cuda=True) as prof:\n",
    "#     with torch.autograd.profiler.record_function(\"model_inference\"):\n",
    "\n",
    "train_accs = []\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "val_losses = []\n",
    "\n",
    "start_timestamp = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "out_filename = \"logs/output_%s.json\" % start_timestamp\n",
    "with open(out_filename, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"model\": repr(model),\n",
    "        \"criterion\": repr(criterion),\n",
    "        \"optimizer\": repr(optimizer)\n",
    "    }, f)\n",
    "    f.write(\"\\n\")\n",
    "    f.flush()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Start timer\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        # Training and validation\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        (train_acc, train_loss, predictions, actuals) = train_model(subset_loader, model, optimizer, criterion, scaler)\n",
    "        (val_acc, val_loss, _, _) = evaluate_model(subset_val_loader, model, criterion)\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        # Print log\n",
    "        time_elapsed = time.perf_counter() - start_time\n",
    "        timestamp = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        print(\"Epoch: %d, Training loss: %.3f, Training accuracy: %.2f%%, Validation loss: %.3f, Validation accuracy: %.2f%%, Learning rate: %f, Time elapsed: %.3f, Timestamp: %s\" %\n",
    "              (epoch, train_loss, 100*train_acc, val_loss, 100*val_acc, lr, time_elapsed, timestamp))\n",
    "\n",
    "        # Save log to disk\n",
    "        json.dump({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"learning_rate\": lr,\n",
    "            \"time_elapsed\": time_elapsed,\n",
    "            \"timestamp\": timestamp\n",
    "        }, f)\n",
    "        f.write(\"\\n\")\n",
    "        f.flush()\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        # Early stopping\n",
    "        if lr < lr_stop:\n",
    "            save_model(model, optimizer, scaler, start_timestamp)\n",
    "            break\n",
    "            \n",
    "# performance = prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20)\n",
    "# print(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure()\n",
    "plt.plot(range(len(train_accs)), train_accs, label=\"Training accuracy\")\n",
    "plt.plot(range(len(val_accs)), val_accs, label=\"Validation accuracy\")\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(train_accs)), train_losses, label=\"Training losses\")\n",
    "plt.plot(range(len(val_losses)), val_losses, label=\"Validation losses\")\n",
    "plt.title(\"Model Losses\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(actuals, range(out_classes+1), density=True, alpha=0.5, label=\"Ground truth\")\n",
    "plt.hist(predictions, range(out_classes+1), density=True, alpha=0.5, label=\"Model output\")\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.title(\"Label Frequencies\")\n",
    "plt.xlabel(\"Output class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "\n",
    "confusion = np.zeros((out_classes, out_classes))\n",
    "for (prediction, actual) in zip(predictions, actuals):\n",
    "    confusion[prediction, actual] += 1\n",
    "confusion\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(confusion, cmap=\"gist_heat\", interpolation=\"nearest\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Model output\")\n",
    "plt.ylabel(\"Ground truth label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
